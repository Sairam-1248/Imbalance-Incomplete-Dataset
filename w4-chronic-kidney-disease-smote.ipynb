{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2005,"sourceType":"datasetVersion","datasetId":1111},{"sourceId":7360491,"sourceType":"datasetVersion","datasetId":4275380},{"sourceId":7366942,"sourceType":"datasetVersion","datasetId":4279856},{"sourceId":7367320,"sourceType":"datasetVersion","datasetId":4280079}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sriusairam/w4-chronic-kidney-disease-smote?scriptVersionId=227031964\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# üí• Chronic Kidney Disease","metadata":{}},{"cell_type":"markdown","source":"üèÜ Problem Statement: Using the data which has 25 features to predict patient with chronic kidney disease","metadata":{}},{"cell_type":"markdown","source":"# Libraries üìñ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nfrom sklearn.impute import SimpleImputer\nimport pandas as pd\nfrom numpy import isnan\nfrom sklearn.preprocessing import LabelEncoder\nfrom numpy import nan\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport plotly.express as px\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# For Model Evaluation\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import roc_curve, auc \nfrom matplotlib import pyplot\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:21:52.938516Z","iopub.execute_input":"2024-02-01T08:21:52.939027Z","iopub.status.idle":"2024-02-01T08:21:52.972959Z","shell.execute_reply.started":"2024-02-01T08:21:52.938993Z","shell.execute_reply":"2024-02-01T08:21:52.972012Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load data üìÅ","metadata":{}},{"cell_type":"code","source":"df_data=pd.read_csv(\"../input/ckdisease/kidney_disease.csv\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-01T08:21:57.233583Z","iopub.execute_input":"2024-02-01T08:21:57.23406Z","iopub.status.idle":"2024-02-01T08:21:57.249684Z","shell.execute_reply.started":"2024-02-01T08:21:57.234021Z","shell.execute_reply":"2024-02-01T08:21:57.248598Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_data","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:21:59.783096Z","iopub.execute_input":"2024-02-01T08:21:59.783564Z","iopub.status.idle":"2024-02-01T08:21:59.829866Z","shell.execute_reply.started":"2024-02-01T08:21:59.783531Z","shell.execute_reply":"2024-02-01T08:21:59.828581Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data analysis üìä","metadata":{}},{"cell_type":"code","source":"df_data.info()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:22:03.158284Z","iopub.execute_input":"2024-02-01T08:22:03.158751Z","iopub.status.idle":"2024-02-01T08:22:03.180218Z","shell.execute_reply.started":"2024-02-01T08:22:03.158717Z","shell.execute_reply":"2024-02-01T08:22:03.178472Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:22:06.230523Z","iopub.execute_input":"2024-02-01T08:22:06.230953Z","iopub.status.idle":"2024-02-01T08:22:06.265874Z","shell.execute_reply.started":"2024-02-01T08:22:06.23092Z","shell.execute_reply":"2024-02-01T08:22:06.263755Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius: 10px; border: #0ea5e9 solid; padding: 15px; background-color: #ffffff00; font-size: 100%; text-align: left;\">üìâ Observation\n      <li><b>Observation One:</b>All Column Names</b> are not user-friendly.</li>\n      <li><b>Observation Two:</b>Following columns values in numeric but reflect as text column\n        <ul>\n          <li>pcv (packed_cell_volume)</li>\n          <li>wc (white_blood_cell_count)</li>\n          <li>rc (red_blood_cell_count)</li>\n        </ul>\n      </li>\n</div>","metadata":{}},{"cell_type":"code","source":"missing = df_data.isnull().sum()\nmissing[missing > 0].sort_values(ascending=False).head(20)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:22:10.208961Z","iopub.execute_input":"2024-02-01T08:22:10.209928Z","iopub.status.idle":"2024-02-01T08:22:10.22411Z","shell.execute_reply.started":"2024-02-01T08:22:10.209883Z","shell.execute_reply":"2024-02-01T08:22:10.222448Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# dropping 'id' column\ndf_data.drop('id', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:22:15.427664Z","iopub.execute_input":"2024-02-01T08:22:15.428073Z","iopub.status.idle":"2024-02-01T08:22:15.435704Z","shell.execute_reply.started":"2024-02-01T08:22:15.428044Z","shell.execute_reply":"2024-02-01T08:22:15.434414Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius: 10px; border: #0ea5e9 solid; padding: 15px; background-color: #ffffff00; font-size: 100%; text-align: left;\"><b>üìâ Observation :</b>\n    <li><b>Observation Three:</b> Certain columns contain <b>missing data</B> that necessitates our attention and management.</li>\n</div>","metadata":{}},{"cell_type":"code","source":"print(f\"dm :- {df_data['dm'].unique()}\")\nprint(f\"cad :- {df_data['cad'].unique()}\")\nprint(f\"classification :- {df_data['classification'].unique()}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:22:18.865215Z","iopub.execute_input":"2024-02-01T08:22:18.865817Z","iopub.status.idle":"2024-02-01T08:22:18.875564Z","shell.execute_reply.started":"2024-02-01T08:22:18.865763Z","shell.execute_reply":"2024-02-01T08:22:18.873748Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius: 10px; border: #0ea5e9 solid; padding: 15px; background-color: #ffffff00; font-size: 100%; text-align: left;\"><b>üìâ Observation :</b>\n    <li><b>Observation Four:</b> There are typo errors in dm (diabetes_mellitus), cad (coronary_artery_disease), and classification (class) columns</li>\n    <li><b>Observation Five:</b> Need to change all column with Text value to Numeric</li>\n</div>","metadata":{}},{"cell_type":"markdown","source":"# Preprocessing ‚öôÔ∏è ","metadata":{}},{"cell_type":"markdown","source":"<li><b>Resolving Observation One:</b> Allocate more user-friendly names to the columns</li>","metadata":{}},{"cell_type":"code","source":"# Notice the unfriendly column names\ndf_data.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:22:22.431691Z","iopub.execute_input":"2024-02-01T08:22:22.432564Z","iopub.status.idle":"2024-02-01T08:22:22.468106Z","shell.execute_reply.started":"2024-02-01T08:22:22.432508Z","shell.execute_reply":"2024-02-01T08:22:22.466287Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_data.columns = ['age', 'blood_pressure', 'specific_gravity', 'albumin', 'sugar', 'red_blood_cells', 'pus_cell',\n              'pus_cell_clumps', 'bacteria', 'blood_glucose_random', 'blood_urea', 'serum_creatinine', 'sodium',\n              'potassium', 'haemoglobin', 'packed_cell_volume', 'white_blood_cell_count', 'red_blood_cell_count',\n              'hypertension', 'diabetes_mellitus', 'coronary_artery_disease', 'appetite', 'peda_edema',\n              'anemia', 'class']","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:22:25.689484Z","iopub.execute_input":"2024-02-01T08:22:25.690147Z","iopub.status.idle":"2024-02-01T08:22:25.697926Z","shell.execute_reply.started":"2024-02-01T08:22:25.690112Z","shell.execute_reply":"2024-02-01T08:22:25.696761Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Friendly column names allocated\ndf_data.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:22:28.618528Z","iopub.execute_input":"2024-02-01T08:22:28.618962Z","iopub.status.idle":"2024-02-01T08:22:28.649868Z","shell.execute_reply.started":"2024-02-01T08:22:28.618929Z","shell.execute_reply":"2024-02-01T08:22:28.648645Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius: 10px; border: #ff001c solid; padding: 15px; margin-top: 15px; background-color: #ffffff00; font-size: \n            100%; text-align: left;\">\n    <b> üö© Column Names renamed successfully</b>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<li><b>Resolving Observation Two:</b> Converting text columns (packed_cell_volume, white_blood_cell_count and red_blood_cell_count) to numeric format</li>","metadata":{}},{"cell_type":"code","source":"# Notice these columns are of datatype Object\ntext_columns = ['packed_cell_volume', 'white_blood_cell_count', 'red_blood_cell_count']\n\nfor column in text_columns:\n    print(f\"{column} -: {df_data[column].dtype}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:22:32.587277Z","iopub.execute_input":"2024-02-01T08:22:32.588319Z","iopub.status.idle":"2024-02-01T08:22:32.594831Z","shell.execute_reply.started":"2024-02-01T08:22:32.588262Z","shell.execute_reply":"2024-02-01T08:22:32.593515Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert text column to numeric column\ndef convert_text_to_numeric_col (dataframe, feature):\n    dataframe[feature] = pd.to_numeric(df_data[feature], errors='coerce')\n\nfor column in text_columns:\n    convert_text_to_numeric_col(df_data, column)\n    print(f\"text_columns: {df_data[column].dtype}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:22:35.426322Z","iopub.execute_input":"2024-02-01T08:22:35.426716Z","iopub.status.idle":"2024-02-01T08:22:35.437102Z","shell.execute_reply.started":"2024-02-01T08:22:35.426687Z","shell.execute_reply":"2024-02-01T08:22:35.435985Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius: 10px; border: #ff001c solid; padding: 15px; margin-top: 15px; background-color: #ffffff00; font-size: \n            100%; text-align: left;\">\n    <b> üö© Column Names (packed_cell_volume, white_blood_cell_count and red_blood_cell_count) converted to numeric successfully</b>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<li><b>Resolving Observation Three:</b> Resolving missing data</li>","metadata":{}},{"cell_type":"code","source":"# Replacing missing values in all numeric columns with mean\n'''def mean_value_imputation(dataframe, feature):\n    mean_value=dataframe[feature].mean()\n    dataframe[feature].fillna(value=mean_value, inplace=True)\n\n# Replacing missing values in all categorical columns with highest frequency data\ndef impute_mode(dataframe, feature):\n    mode = dataframe[feature].mode()[0]\n    dataframe[feature] = dataframe[feature].fillna(mode)  '''","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:22:40.638764Z","iopub.execute_input":"2024-02-01T08:22:40.639712Z","iopub.status.idle":"2024-02-01T08:22:40.648325Z","shell.execute_reply.started":"2024-02-01T08:22:40.639674Z","shell.execute_reply":"2024-02-01T08:22:40.647149Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Obtaining columns names of all numerical features\n'''num_columns = [col for col in df_data.columns if df_data[col].dtype != 'object']\n\n# Assigning random number to all missing data in numeric columns\nfor column_name in num_columns:\n    mean_value_imputation(df_data,column_name)'''","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:22:43.87719Z","iopub.execute_input":"2024-02-01T08:22:43.878483Z","iopub.status.idle":"2024-02-01T08:22:43.886923Z","shell.execute_reply.started":"2024-02-01T08:22:43.87843Z","shell.execute_reply":"2024-02-01T08:22:43.885722Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Obtaining columns names of all categorized features\n'''cat_columns = [col for col in df_data.columns if df_data[col].dtype == 'object']\nimpute_mode(df_data,\"blood_pressure\")\n\n# Assigning highest frequency to all missing data in categorical columns\nfor column_name in cat_columns:\n    impute_mode(df_data,column_name)'''","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:22:46.568645Z","iopub.execute_input":"2024-02-01T08:22:46.570064Z","iopub.status.idle":"2024-02-01T08:22:46.578613Z","shell.execute_reply.started":"2024-02-01T08:22:46.569981Z","shell.execute_reply":"2024-02-01T08:22:46.577052Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing = df_data.isnull().sum()\nmissing[missing > 0].sort_values(ascending=False).head(20)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:22:51.912109Z","iopub.execute_input":"2024-02-01T08:22:51.91259Z","iopub.status.idle":"2024-02-01T08:22:51.930131Z","shell.execute_reply.started":"2024-02-01T08:22:51.912553Z","shell.execute_reply":"2024-02-01T08:22:51.928442Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius: 10px; border: #ff001c solid; padding: 15px; margin-top: 15px; background-color: #ffffff00; font-size: \n            100%; text-align: left;\">\n    <b> üö© All missing values have been filled up</b>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<li><b>Resolving Observation Four:</b> Typo Errors in dm (diabetes_mellitus), cad (coronary_artery_disease), and classification (class) columns</li>","metadata":{}},{"cell_type":"code","source":"print(f\"diabetes_mellitus :- {df_data['diabetes_mellitus'].unique()}\")\nprint(f\"coronary_artery_disease :- {df_data['coronary_artery_disease'].unique()}\")\nprint(f\"class :- {df_data['class'].unique()}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:23:34.485126Z","iopub.execute_input":"2024-02-01T08:23:34.485722Z","iopub.status.idle":"2024-02-01T08:23:34.496289Z","shell.execute_reply.started":"2024-02-01T08:23:34.485684Z","shell.execute_reply":"2024-02-01T08:23:34.494697Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_data['diabetes_mellitus'] = df_data['diabetes_mellitus'].replace(to_replace = {' yes':'yes', '\\tno':'no', '\\tyes':'yes'})\ndf_data['coronary_artery_disease'] = df_data['coronary_artery_disease'].replace(to_replace = '\\tno', value='no')\ndf_data['class'] = df_data['class'].replace(to_replace = {'ckd\\t': 'ckd', 'notckd': 'not ckd'})","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:23:38.546595Z","iopub.execute_input":"2024-02-01T08:23:38.547053Z","iopub.status.idle":"2024-02-01T08:23:38.558946Z","shell.execute_reply.started":"2024-02-01T08:23:38.547022Z","shell.execute_reply":"2024-02-01T08:23:38.557305Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"diabetes_mellitus'] :- {df_data['diabetes_mellitus'].unique()}\")\nprint(f\"coronary_artery_disease :- {df_data['coronary_artery_disease'].unique()}\")\nprint(f\"class :- {df_data['class'].unique()}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:23:42.041837Z","iopub.execute_input":"2024-02-01T08:23:42.042305Z","iopub.status.idle":"2024-02-01T08:23:42.050955Z","shell.execute_reply.started":"2024-02-01T08:23:42.04227Z","shell.execute_reply":"2024-02-01T08:23:42.049292Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius: 10px; border: #ff001c solid; padding: 15px; margin-top: 15px; background-color: #ffffff00; font-size: \n            100%; text-align: left;\">\n    <b> üö© All typo errors for the three columns rectified</b>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<li><b>Resolving Observation Five:</b> Feature Encoding - Need to change all column with Text value to Numeric</li>","metadata":{}},{"cell_type":"code","source":"df_data['class'] = df_data['class'].map({'ckd': 1, 'not ckd': 0})\ndf_data['red_blood_cells'] = df_data['red_blood_cells'].map({'normal': 1, 'abnormal': 0})\ndf_data['pus_cell'] = df_data['pus_cell'].map({'normal': 1, 'abnormal': 0})\ndf_data['pus_cell_clumps'] = df_data['pus_cell_clumps'].map({'present': 1, 'notpresent': 0})\ndf_data['bacteria'] = df_data['bacteria'].map({'present': 1, 'notpresent': 0})\ndf_data['hypertension'] = df_data['hypertension'].map({'yes': 1, 'no': 0})\ndf_data['diabetes_mellitus'] = df_data['diabetes_mellitus'].map({'yes': 1, 'no': 0})\ndf_data['coronary_artery_disease'] = df_data['coronary_artery_disease'].map({'yes': 1, 'no': 0}) \ndf_data['appetite'] = df_data['appetite'].map({'good': 1, 'poor': 0})\ndf_data['peda_edema'] = df_data['peda_edema'].map({'yes': 1, 'no': 0})\ndf_data['anemia'] = df_data['anemia'].map({'yes': 1, 'no': 0})","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:23:48.108209Z","iopub.execute_input":"2024-02-01T08:23:48.108744Z","iopub.status.idle":"2024-02-01T08:23:48.137017Z","shell.execute_reply.started":"2024-02-01T08:23:48.108704Z","shell.execute_reply":"2024-02-01T08:23:48.135475Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for column in text_columns:\n    convert_text_to_numeric_col(df_data, column)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:24:08.603818Z","iopub.execute_input":"2024-02-01T08:24:08.604387Z","iopub.status.idle":"2024-02-01T08:24:08.613325Z","shell.execute_reply.started":"2024-02-01T08:24:08.604351Z","shell.execute_reply":"2024-02-01T08:24:08.611206Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:24:11.899778Z","iopub.execute_input":"2024-02-01T08:24:11.900304Z","iopub.status.idle":"2024-02-01T08:24:11.94669Z","shell.execute_reply.started":"2024-02-01T08:24:11.900267Z","shell.execute_reply":"2024-02-01T08:24:11.945405Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_data.to_csv('newmodifieddataset.csv')","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:24:32.397752Z","iopub.execute_input":"2024-02-01T08:24:32.398249Z","iopub.status.idle":"2024-02-01T08:24:32.419704Z","shell.execute_reply.started":"2024-02-01T08:24:32.398203Z","shell.execute_reply":"2024-02-01T08:24:32.417704Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_data","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:08:41.963573Z","iopub.execute_input":"2024-02-01T08:08:41.964035Z","iopub.status.idle":"2024-02-01T08:08:42.033508Z","shell.execute_reply.started":"2024-02-01T08:08:41.964001Z","shell.execute_reply":"2024-02-01T08:08:42.032122Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_data=pd.read_csv(\"/kaggle/input/kidney-modified-dataset/Kidney_dataset_Modified.csv\").iloc[:, 1:]","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:24:41.350483Z","iopub.execute_input":"2024-02-01T08:24:41.350901Z","iopub.status.idle":"2024-02-01T08:24:41.368053Z","shell.execute_reply.started":"2024-02-01T08:24:41.35087Z","shell.execute_reply":"2024-02-01T08:24:41.366365Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Apply EM technique","metadata":{}},{"cell_type":"code","source":"pip install impyute","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:35:14.938458Z","iopub.execute_input":"2024-02-01T08:35:14.938938Z","iopub.status.idle":"2024-02-01T08:35:29.59235Z","shell.execute_reply.started":"2024-02-01T08:35:14.938904Z","shell.execute_reply":"2024-02-01T08:35:29.590786Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import impyute as impy","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:35:36.664171Z","iopub.execute_input":"2024-02-01T08:35:36.664641Z","iopub.status.idle":"2024-02-01T08:35:36.683431Z","shell.execute_reply.started":"2024-02-01T08:35:36.664608Z","shell.execute_reply":"2024-02-01T08:35:36.681893Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"imputeddata=impy.em(df_data.values, loops=1000)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:35:55.905282Z","iopub.execute_input":"2024-02-01T08:35:55.905709Z","iopub.status.idle":"2024-02-01T08:35:56.246247Z","shell.execute_reply.started":"2024-02-01T08:35:55.905677Z","shell.execute_reply":"2024-02-01T08:35:56.245191Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_data=pd.DataFrame(imputeddata) ","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:36:10.234172Z","iopub.execute_input":"2024-02-01T08:36:10.235253Z","iopub.status.idle":"2024-02-01T08:36:10.241888Z","shell.execute_reply.started":"2024-02-01T08:36:10.235213Z","shell.execute_reply":"2024-02-01T08:36:10.240266Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#df_data=df_data.fillna(0)\n#df_data.to_csv(\"newwdata_fi10test17.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:04:11.310375Z","iopub.execute_input":"2024-02-01T08:04:11.310783Z","iopub.status.idle":"2024-02-01T08:04:11.317966Z","shell.execute_reply.started":"2024-02-01T08:04:11.310746Z","shell.execute_reply":"2024-02-01T08:04:11.316448Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_data.columns = ['age', 'blood_pressure', 'specific_gravity', 'albumin', 'sugar', 'red_blood_cells', 'pus_cell',\n              'pus_cell_clumps', 'bacteria', 'blood_glucose_random', 'blood_urea', 'serum_creatinine', 'sodium',\n              'potassium', 'haemoglobin', 'packed_cell_volume', 'white_blood_cell_count', 'red_blood_cell_count',\n              'hypertension', 'diabetes_mellitus', 'coronary_artery_disease', 'appetite', 'peda_edema',\n              'anemia', 'class']","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:56:19.527381Z","iopub.execute_input":"2024-02-01T08:56:19.527934Z","iopub.status.idle":"2024-02-01T08:56:19.537683Z","shell.execute_reply.started":"2024-02-01T08:56:19.52789Z","shell.execute_reply":"2024-02-01T08:56:19.536447Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_data","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:56:22.944025Z","iopub.execute_input":"2024-02-01T08:56:22.94539Z","iopub.status.idle":"2024-02-01T08:56:22.995017Z","shell.execute_reply.started":"2024-02-01T08:56:22.945321Z","shell.execute_reply":"2024-02-01T08:56:22.993427Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_data.info()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:56:26.934889Z","iopub.execute_input":"2024-02-01T08:56:26.935385Z","iopub.status.idle":"2024-02-01T08:56:26.9524Z","shell.execute_reply.started":"2024-02-01T08:56:26.935349Z","shell.execute_reply":"2024-02-01T08:56:26.950923Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius: 10px; border: #ff001c solid; padding: 15px; margin-top: 15px; background-color: #ffffff00; font-size: \n            100%; text-align: left;\">\n    <b> üö© All columns assigned to numeric</b>\n</div>","metadata":{}},{"cell_type":"markdown","source":"# Modeling ü™Ñ","metadata":{"execution":{"iopub.status.busy":"2023-08-22T02:34:09.351099Z","iopub.execute_input":"2023-08-22T02:34:09.351547Z","iopub.status.idle":"2023-08-22T02:34:09.356464Z","shell.execute_reply.started":"2023-08-22T02:34:09.35151Z","shell.execute_reply":"2023-08-22T02:34:09.355272Z"}}},{"cell_type":"markdown","source":"## Splitting Dataset","metadata":{}},{"cell_type":"code","source":"# Define Class as Target Variable, and the rest as feature variable\nX = df_data.drop(\"class\", axis=1)     # everything except 'class' column\ny = df_data['class']\n\n# Define the train dataset as 70% and test dataset as 30%\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state = 1)\n\n# Confirm that the records returned for Train is about 70% and Test is about 30%\nprint(f\"'X' shape: {X_train.shape}\")\nprint(f\"'y' shape: {X_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:56:31.090256Z","iopub.execute_input":"2024-02-01T08:56:31.091358Z","iopub.status.idle":"2024-02-01T08:56:31.103355Z","shell.execute_reply.started":"2024-02-01T08:56:31.091267Z","shell.execute_reply":"2024-02-01T08:56:31.102111Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Apply Smote to balance the samples ","metadata":{}},{"cell_type":"code","source":"!pip install -U imbalanced-learn","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:41:46.872303Z","iopub.execute_input":"2024-02-01T08:41:46.87282Z","iopub.status.idle":"2024-02-01T08:42:00.900942Z","shell.execute_reply.started":"2024-02-01T08:41:46.872785Z","shell.execute_reply":"2024-02-01T08:42:00.899272Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Before OverSampling, counts of label '1': {}\".format(sum(Y_train == 1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(Y_train == 0)))","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:56:37.033367Z","iopub.execute_input":"2024-02-01T08:56:37.034508Z","iopub.status.idle":"2024-02-01T08:56:37.042194Z","shell.execute_reply.started":"2024-02-01T08:56:37.034464Z","shell.execute_reply":"2024-02-01T08:56:37.040843Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\noversample = SMOTE()\nX_train, Y_train = oversample.fit_resample(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:57:12.709054Z","iopub.execute_input":"2024-02-01T08:57:12.709889Z","iopub.status.idle":"2024-02-01T08:57:12.727131Z","shell.execute_reply.started":"2024-02-01T08:57:12.709847Z","shell.execute_reply":"2024-02-01T08:57:12.725447Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('After OverSampling, the shape of train_X: {}'.format(X_train.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(Y_train.shape))","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:57:29.436469Z","iopub.execute_input":"2024-02-01T08:57:29.437677Z","iopub.status.idle":"2024-02-01T08:57:29.444779Z","shell.execute_reply.started":"2024-02-01T08:57:29.437607Z","shell.execute_reply":"2024-02-01T08:57:29.44312Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"After OverSampling, counts of label '1': {}\".format(sum(Y_train == 1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(Y_train == 0)))","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:57:51.749527Z","iopub.execute_input":"2024-02-01T08:57:51.749956Z","iopub.status.idle":"2024-02-01T08:57:51.758486Z","shell.execute_reply.started":"2024-02-01T08:57:51.749926Z","shell.execute_reply":"2024-02-01T08:57:51.757231Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install wget \n\nimport wget\nwget.download('https://raw.githubusercontent.com/BorisMuzellec/MissingDataOT/master/utils.py')\n\nimport numpy as np\nimport pandas as pd\nfrom utils import *\nimport torch\nimport seaborn as sns\n\nimport pandas as pd\nimport missingno as msno\nhar = df_data\n#har1=har.iloc[:,:-1]\nhar1=har\n#msno.heatmap(har1)\n#msno.dendrogram(har1)\nhar1=har1.values\n#print('Shape Test:\\t{}\\n'.format(har1.shape))\nhar1\n\n\nhar1=har1.astype(float)\nhar1\n\n\ndef produce_NA(har1, p_miss, mecha=\"MAR\", opt=\"logistic\", p_obs=None, q=None):\n    to_torch = torch.is_tensor(har1) ## output a pytorch tensor, or a numpy array\n    if not to_torch:\n        har1 = har1.astype(np.float32)\n        har1 = torch.from_numpy(har1)\n    \n    if mecha == \"MAR\" and opt == \"logistic\":\n        mask = MAR_mask(har1, p_miss, p_obs).double()\n        print(\"mask values\")\n        print(mask)\n    elif mecha == \"MNAR\" and opt == \"logistic\":\n        mask = MNAR_mask_logistic(har1, p_miss, p_obs).double()\n    elif mecha == \"MNAR\" and opt == \"quantile\":\n        mask = MNAR_mask_quantiles(har1, p_miss, q, 1-p_obs).double()\n    elif mecha == \"MNAR\" and opt == \"selfmasked\":\n        mask = MNAR_self_mask_logistic(har1, p_miss).double()\n    else:\n        mask = (torch.rand(har1.shape) < p_miss).double()\n     \n    \n    har1_nas = har1.clone()\n    har1_nas[mask.bool()] = np.nan\n    \n    \n    \n    \n    return {'har1_init': har1.double(), 'har1_incomp': har1_nas.double(), 'mask': mask}\n\n\n\n\norig_dataset=df_data.drop('class',axis=1)\norig_dataset\nimport pandas as pd\nimport missingno as msno\nhar = orig_dataset\n#har1=har.iloc[:,:-1]\nhar1=har\n#msno.heatmap(har1)\n#msno.dendrogram(har1)\nhar1=har1.values\n#print('Shape Test:\\t{}\\n'.format(har1.shape))\nhar1\n\n\nhar1=har1.astype(float)\nhar1\n\nhar1_miss_mar = produce_NA(har1=har1, p_miss=0.99, mecha=\"MAR\", p_obs=0.5)\n \n\n\n#print(har1_mar) \nprint(har1_miss_mar['har1_init'])\nhar1_mar = har1_miss_mar['har1_incomp']\nprint(har1_mar)  \n#har1_mar = pd.DataFrame.from_dict(har1_mar)\nR_mcar = har1_miss_mar['mask']\nprint(R_mcar)\n \nprint(\"Percentage of generated missing values: \", (R_mcar.sum()).numpy()/np.prod(R_mcar.size())*100, \" %\")\n\nprint(har1_mar)\nprint('Shape Test:\\t{}\\n'.format(har1_mar.shape))\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\n \nx_np = har1_mar.numpy()\nx_df = pd.DataFrame(x_np)\nx_df.to_csv('10mar_test.csv')\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\n\ntdf = pd.read_csv(\"./10mar_test.csv\").iloc[:, 1:]\ntdf.head()\n\ntdf.shape\n# Get null values and dataframe information  check for any missing data in data sets\nprint('Null Values In DataFrame: {}\\n'.format(tdf.isna().sum().sum()))\ntdf.info()\ntdf=tdf.fillna(0)\ntdf.to_csv(\"data_fil10.csv\")\n\ntdf\n\n\n\ntestdata = pd.read_csv('./10mar_test.csv').iloc[:, 1:] \n \ntestdata.head(10)\n#testdata=testdata.drop(['16'],axis=1)\ntestdata\n\nprint('Null Values In DataFrame: {}\\n'.format(testdata.isna().sum().sum()))\ntestdata.isnull().sum(0)\n\ntestdata=testdata.fillna(0)\ntestdata\nfrom sklearn.model_selection import train_test_split\n#kfold =sklearn.model_selection.KFold(n_splits=10)\n\nimport pandas as pd\n \nfrom sklearn.metrics import log_loss\n#seed=7\n#\n\n# importing voting classifier\nfrom sklearn.ensemble import VotingClassifier\n \n    \n#Edata10Mar = supp10_data2\nEdata10Mar =testdata \nEdata10Mar\nX = Edata10Mar  \ny= df_data[\"class\"] \nX\nfeature_cols = list(X) \ny\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = None)\nclf_dtc5 = DecisionTreeClassifier()\nclf_dtc5.fit(X_train, y_train)\nfeature_cols = list(X) \nimport six\nimport sys\nsys.modules['sklearn.externals.six'] = six\n#from six imporaccuracy_scoret StringIO\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(clf_dtc5, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True, feature_names = feature_cols,class_names=['0','1','2'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('ClassifierIoT1.png')\ni=Image(graph.create_png())\ni\n\n\n\ny_pred = clf_dtc5.predict(X_test)\nprint(y_pred)\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(y_test, y_pred)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-01T09:09:42.331366Z","iopub.execute_input":"2024-02-01T09:09:42.331886Z","iopub.status.idle":"2024-02-01T09:09:57.285726Z","shell.execute_reply.started":"2024-02-01T09:09:42.331847Z","shell.execute_reply":"2024-02-01T09:09:57.284167Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Models","metadata":{}},{"cell_type":"code","source":"# Random Forest\nclf_rand_forest = RandomForestClassifier()\nclf_rand_forest.fit(X_train, Y_train)\n\n# SVM\nclf_svm = svm.SVC(kernel='linear')\nclf_svm.fit(X_train, Y_train)\n\n# KNN\nclf_knn = KNeighborsClassifier(n_neighbors=5)\nclf_knn.fit(X_train, Y_train)\n\n# Decision Tree\nclf_dtc = DecisionTreeClassifier()\nclf_dtc.fit(X_train, Y_train)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:46:11.494681Z","iopub.execute_input":"2024-02-01T08:46:11.495175Z","iopub.status.idle":"2024-02-01T08:46:15.900524Z","shell.execute_reply.started":"2024-02-01T08:46:11.495142Z","shell.execute_reply":"2024-02-01T08:46:15.899218Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model evaluation","metadata":{}},{"cell_type":"code","source":"# Printing of Model Evaluation Report\ndef print_std_model_evaulation_rpt(Y_test, Y_pred):\n    print(classification_report(Y_test, Y_pred))\n    print(f\"mean_absolute_error :- {mean_absolute_error(Y_test,Y_pred)}\")\n    print(f\"mean_absolute_error :- {mean_squared_error(Y_test,Y_pred, squared=False)}\")\n    cm1 = confusion_matrix(Y_test, Y_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm1,display_labels=clf_rand_forest.classes_)\n    disp.plot()\n    plt.show()\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:46:20.565463Z","iopub.execute_input":"2024-02-01T08:46:20.565864Z","iopub.status.idle":"2024-02-01T08:46:20.574643Z","shell.execute_reply.started":"2024-02-01T08:46:20.565834Z","shell.execute_reply":"2024-02-01T08:46:20.572811Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Random Forest","metadata":{}},{"cell_type":"markdown","source":"#### Classification Report","metadata":{}},{"cell_type":"code","source":"# Random Forest\nY_pred = clf_rand_forest.predict(X_test)\nrand_forest_acc = accuracy_score(Y_test, Y_pred)\nprint_std_model_evaulation_rpt(Y_test, Y_pred)\n\n\nprint(rand_forest_acc)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:46:24.076231Z","iopub.execute_input":"2024-02-01T08:46:24.076726Z","iopub.status.idle":"2024-02-01T08:46:24.473075Z","shell.execute_reply.started":"2024-02-01T08:46:24.076673Z","shell.execute_reply":"2024-02-01T08:46:24.471782Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score,f1_score\nimport sklearn.metrics as metrics\n#print(classification_report(Y_test, y_pred))\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(Y_test, Y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:46:29.075677Z","iopub.execute_input":"2024-02-01T08:46:29.076145Z","iopub.status.idle":"2024-02-01T08:46:29.109487Z","shell.execute_reply.started":"2024-02-01T08:46:29.07611Z","shell.execute_reply":"2024-02-01T08:46:29.107787Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### SVM","metadata":{}},{"cell_type":"markdown","source":"#### Classification Report","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nX = df_data.drop(\"class\", axis=1)     # everything except 'class' column\ny = df_data['class']\n# Define the train dataset as 70% and test dataset as 30%\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state = 1)\n\nclf_svm = svm.SVC(kernel='linear') #0.97\n# clf_svm = SVC(gamma='auto') # 0.59\n# clf_svm = SVC(kernel='poly') # 0.59\n# clf_svm = SVC(kernel='rbf',gamma=0.01) # 0.59\n# clf_svm.fit(X_test,Y_test)\nclf_svm.fit(X_train, Y_train)\n\nY_pred = clf_svm.predict(X_test)\nsvm_acc = accuracy_score(Y_test, Y_pred)\nprint_std_model_evaulation_rpt(Y_test, Y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:46:38.449143Z","iopub.execute_input":"2024-02-01T08:46:38.44964Z","iopub.status.idle":"2024-02-01T08:46:45.18702Z","shell.execute_reply.started":"2024-02-01T08:46:38.449608Z","shell.execute_reply":"2024-02-01T08:46:45.185968Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score,f1_score\nimport sklearn.metrics as metrics\n#print(classification_report(Y_test, y_pred))\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(Y_test, Y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:46:48.740614Z","iopub.execute_input":"2024-02-01T08:46:48.74103Z","iopub.status.idle":"2024-02-01T08:46:48.770949Z","shell.execute_reply.started":"2024-02-01T08:46:48.740999Z","shell.execute_reply":"2024-02-01T08:46:48.769411Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### KNN\nclf_knn = KNeighborsClassifier(n_neighbors=5)\nclf_knn.fit(X_train, Y_train)\nY_pred = clf_knn.predict(X_test)\nknn_acc = accuracy_score(Y_test, Y_pred)\nprint_std_model_evaulation_rpt(Y_test, Y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:46:53.014487Z","iopub.execute_input":"2024-02-01T08:46:53.014984Z","iopub.status.idle":"2024-02-01T08:46:53.490218Z","shell.execute_reply.started":"2024-02-01T08:46:53.014947Z","shell.execute_reply":"2024-02-01T08:46:53.488247Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score,f1_score\nimport sklearn.metrics as metrics\n#print(classification_report(Y_test, y_pred))\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(Y_test, Y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:47:08.868142Z","iopub.execute_input":"2024-02-01T08:47:08.868593Z","iopub.status.idle":"2024-02-01T08:47:08.902015Z","shell.execute_reply.started":"2024-02-01T08:47:08.868562Z","shell.execute_reply":"2024-02-01T08:47:08.900616Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Decision Tree\nY_pred = clf_dtc.predict(X_test)\ndtc_acc = accuracy_score(Y_test, Y_pred)\nprint_std_model_evaulation_rpt(Y_test, Y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:47:12.57549Z","iopub.execute_input":"2024-02-01T08:47:12.575908Z","iopub.status.idle":"2024-02-01T08:47:12.90562Z","shell.execute_reply.started":"2024-02-01T08:47:12.575878Z","shell.execute_reply":"2024-02-01T08:47:12.904084Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score,f1_score\nimport sklearn.metrics as metrics\n#print(classification_report(Y_test, y_pred))\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(Y_test, Y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:47:17.770356Z","iopub.execute_input":"2024-02-01T08:47:17.770797Z","iopub.status.idle":"2024-02-01T08:47:17.80687Z","shell.execute_reply.started":"2024-02-01T08:47:17.770766Z","shell.execute_reply":"2024-02-01T08:47:17.804927Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models = pd.DataFrame({\n    'Model' : [ 'Random Forest Classifier', 'SVM Classifier', 'KNN Classifier', 'Decision Tree Classifier'],\n    'Score' : [rand_forest_acc, svm_acc,knn_acc, dtc_acc]\n})\n\n\nsorted_models = models.sort_values(by = 'Score', ascending = True)\n\nfig = px.bar(data_frame = sorted_models, x = 'Score', y = 'Model',\n       title = 'Models Comparison')\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:47:22.118836Z","iopub.execute_input":"2024-02-01T08:47:22.120107Z","iopub.status.idle":"2024-02-01T08:47:24.287316Z","shell.execute_reply.started":"2024-02-01T08:47:22.120051Z","shell.execute_reply":"2024-02-01T08:47:24.285132Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install astor\n","metadata":{"execution":{"iopub.status.busy":"2024-01-09T04:30:00.015126Z","iopub.execute_input":"2024-01-09T04:30:00.015673Z","iopub.status.idle":"2024-01-09T04:30:16.946809Z","shell.execute_reply.started":"2024-01-09T04:30:00.015635Z","shell.execute_reply":"2024-01-09T04:30:16.945216Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install skompiler\n","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:47:41.360462Z","iopub.execute_input":"2024-02-01T08:47:41.360893Z","iopub.status.idle":"2024-02-01T08:47:58.891571Z","shell.execute_reply.started":"2024-02-01T08:47:41.360861Z","shell.execute_reply":"2024-02-01T08:47:58.88981Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install pydotplus","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:47:58.895158Z","iopub.execute_input":"2024-02-01T08:47:58.895779Z","iopub.status.idle":"2024-02-01T08:48:16.106685Z","shell.execute_reply.started":"2024-02-01T08:47:58.895706Z","shell.execute_reply":"2024-02-01T08:48:16.105082Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df_data.drop(\"class\", axis=1)     # everything except 'class' column\ny = df_data['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = None)\nclf_dtc.fit(X_train, y_train)\nfeature_cols = list(X) \nimport six\nimport sys\nsys.modules['sklearn.externals.six'] = six\n#from six imporaccuracy_scoret StringIO\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(clf_dtc, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True, feature_names = feature_cols,class_names=['0','1','2'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('ClassifierIoT1.png')\ni=Image(graph.create_png())\ni","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:48:16.10845Z","iopub.execute_input":"2024-02-01T08:48:16.108885Z","iopub.status.idle":"2024-02-01T08:48:16.796167Z","shell.execute_reply.started":"2024-02-01T08:48:16.108847Z","shell.execute_reply":"2024-02-01T08:48:16.795141Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = clf_dtc.predict(X_test)\nprint(y_pred)\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-02-01T08:48:16.798736Z","iopub.execute_input":"2024-02-01T08:48:16.799181Z","iopub.status.idle":"2024-02-01T08:48:16.834131Z","shell.execute_reply.started":"2024-02-01T08:48:16.799144Z","shell.execute_reply":"2024-02-01T08:48:16.833078Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create a subspace classifiers \n","metadata":{}},{"cell_type":"code","source":"df_data1=pd.read_csv(\"/kaggle/input/kidney-modified-dataset/Kidney_dataset_Modified.csv\").iloc[:, 1:]","metadata":{"execution":{"iopub.status.busy":"2024-01-09T04:30:52.212361Z","iopub.execute_input":"2024-01-09T04:30:52.212793Z","iopub.status.idle":"2024-01-09T04:30:52.231073Z","shell.execute_reply.started":"2024-01-09T04:30:52.212752Z","shell.execute_reply":"2024-01-09T04:30:52.229629Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_data1","metadata":{"execution":{"iopub.status.busy":"2024-01-09T04:30:52.233186Z","iopub.execute_input":"2024-01-09T04:30:52.235336Z","iopub.status.idle":"2024-01-09T04:30:52.290374Z","shell.execute_reply.started":"2024-01-09T04:30:52.235285Z","shell.execute_reply":"2024-01-09T04:30:52.289074Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_data1=df_data1.fillna(0)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T04:30:52.29211Z","iopub.execute_input":"2024-01-09T04:30:52.292637Z","iopub.status.idle":"2024-01-09T04:30:52.300352Z","shell.execute_reply.started":"2024-01-09T04:30:52.29259Z","shell.execute_reply":"2024-01-09T04:30:52.298919Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns = ['age','blood_pressure','sugar','red_blood_cells ',\n'pus_cell','pus_cell_clumps','bacteria','blood_glucose_random','blood_urea','potassium','white_blood_cell_count','red_blood_cell_count','hypertension','coronary_artery_disease','peda_edema','class']\ndata2 = pd.DataFrame(df_data1, columns=columns)\ndata2","metadata":{"execution":{"iopub.status.busy":"2024-01-08T08:50:22.033449Z","iopub.execute_input":"2024-01-08T08:50:22.033832Z","iopub.status.idle":"2024-01-08T08:50:22.070972Z","shell.execute_reply.started":"2024-01-08T08:50:22.0338Z","shell.execute_reply":"2024-01-08T08:50:22.070021Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data2=data2.fillna(0)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T08:52:52.46331Z","iopub.execute_input":"2024-01-08T08:52:52.463715Z","iopub.status.idle":"2024-01-08T08:52:52.469574Z","shell.execute_reply.started":"2024-01-08T08:52:52.463684Z","shell.execute_reply":"2024-01-08T08:52:52.468619Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = data2.drop(\"class\", axis=1)     # everything except 'class' column\ny = data2['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = None)\nclf_dtc2 = DecisionTreeClassifier()\nclf_dtc2.fit(X_train, y_train)\nfeature_cols = list(X) \nimport six\nimport sys\nsys.modules['sklearn.externals.six'] = six\n#from six imporaccuracy_scoret StringIO\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(clf_dtc2, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True, feature_names = feature_cols,class_names=['0','1','2'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('ClassifierIoT1.png')\ni=Image(graph.create_png())\ni","metadata":{"execution":{"iopub.status.busy":"2024-01-08T08:52:54.469787Z","iopub.execute_input":"2024-01-08T08:52:54.470138Z","iopub.status.idle":"2024-01-08T08:52:55.625002Z","shell.execute_reply.started":"2024-01-08T08:52:54.470113Z","shell.execute_reply":"2024-01-08T08:52:55.624091Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = clf_dtc2.predict(X_test)\nprint(y_pred)\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T08:53:06.927972Z","iopub.execute_input":"2024-01-08T08:53:06.928353Z","iopub.status.idle":"2024-01-08T08:53:06.954008Z","shell.execute_reply.started":"2024-01-08T08:53:06.928322Z","shell.execute_reply":"2024-01-08T08:53:06.95312Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modified Dataset -2","metadata":{}},{"cell_type":"code","source":"columns = ['specific_gravity','albumin','serum_creatinine','sodium',\n'haemoglobin','packed_cell_volume','diabetes_mellitus','appetite','anemia','age','blood_pressure','red_blood_cells','pus_cell','pus_cell_clumps','bacteria','coronary_artery_disease','class']\ndata3 = pd.DataFrame(df_data1, columns=columns)\ndata3\n","metadata":{"execution":{"iopub.status.busy":"2024-01-08T09:29:12.833386Z","iopub.execute_input":"2024-01-08T09:29:12.834277Z","iopub.status.idle":"2024-01-08T09:29:12.87072Z","shell.execute_reply.started":"2024-01-08T09:29:12.834245Z","shell.execute_reply":"2024-01-08T09:29:12.869776Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data3=data3.fillna(0)\n\nX = data3.drop(\"class\", axis=1)     # everything except 'class' column\ny = data3['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = None)\nclf_dtc3 = DecisionTreeClassifier()\nclf_dtc3.fit(X_train, y_train)\nfeature_cols = list(X) \nimport six\nimport sys\nsys.modules['sklearn.externals.six'] = six\n#from six imporaccuracy_scoret StringIO\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(clf_dtc3, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True, feature_names = feature_cols,class_names=['0','1','2'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('ClassifierIoT1.png')\ni=Image(graph.create_png())\ni\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-08T09:32:43.296359Z","iopub.execute_input":"2024-01-08T09:32:43.29675Z","iopub.status.idle":"2024-01-08T09:32:43.743898Z","shell.execute_reply.started":"2024-01-08T09:32:43.296707Z","shell.execute_reply":"2024-01-08T09:32:43.742909Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = clf_dtc3.predict(X_test)\nprint(y_pred)\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(y_test, y_pred)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-08T09:32:48.634332Z","iopub.execute_input":"2024-01-08T09:32:48.635064Z","iopub.status.idle":"2024-01-08T09:32:48.659102Z","shell.execute_reply.started":"2024-01-08T09:32:48.635029Z","shell.execute_reply":"2024-01-08T09:32:48.658036Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modified Datast 3\n","metadata":{}},{"cell_type":"code","source":"columns = ['specific_gravity','appetite','anemia','age',\n'blood_pressure','red_blood_cells','pus_cell','pus_cell_clumps ','bacteria','coronary_artery_disease  ','blood_pressure','red_blood_cells','pus_cell','pus_cell_clumps','bacteria','coronary_artery_disease','class']\ndata4 = pd.DataFrame(df_data1, columns=columns)\ndata4\n","metadata":{"execution":{"iopub.status.busy":"2024-01-08T09:43:23.338945Z","iopub.execute_input":"2024-01-08T09:43:23.339897Z","iopub.status.idle":"2024-01-08T09:43:23.381473Z","shell.execute_reply.started":"2024-01-08T09:43:23.339856Z","shell.execute_reply":"2024-01-08T09:43:23.380562Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data4=data4.fillna(0)\n\nX = data4.drop(\"class\", axis=1)     # everything except 'class' column\ny = data4['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = None)\nclf_dtc4 = DecisionTreeClassifier()\nclf_dtc4.fit(X_train, y_train)\nfeature_cols = list(X) \nimport six\nimport sys\nsys.modules['sklearn.externals.six'] = six\n#from six imporaccuracy_scoret StringIO\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(clf_dtc4, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True, feature_names = feature_cols,class_names=['0','1','2'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('ClassifierIoT3.png')\ni=Image(graph.create_png())\ni","metadata":{"execution":{"iopub.status.busy":"2024-01-08T09:46:19.633502Z","iopub.execute_input":"2024-01-08T09:46:19.634233Z","iopub.status.idle":"2024-01-08T09:46:20.175713Z","shell.execute_reply.started":"2024-01-08T09:46:19.634198Z","shell.execute_reply":"2024-01-08T09:46:20.174699Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = clf_dtc4.predict(X_test)\nprint(y_pred)\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T09:46:33.142212Z","iopub.execute_input":"2024-01-08T09:46:33.143101Z","iopub.status.idle":"2024-01-08T09:46:33.169763Z","shell.execute_reply.started":"2024-01-08T09:46:33.143068Z","shell.execute_reply":"2024-01-08T09:46:33.168793Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modifed Dataset -4","metadata":{}},{"cell_type":"code","source":"columns = ['appetite','anemia','pus_cell_clumps ','bacteria','coronary_artery_disease','class']\ndata5 = pd.DataFrame(df_data1, columns=columns)\ndata5","metadata":{"execution":{"iopub.status.busy":"2024-01-08T09:50:19.614337Z","iopub.execute_input":"2024-01-08T09:50:19.615122Z","iopub.status.idle":"2024-01-08T09:50:19.635543Z","shell.execute_reply.started":"2024-01-08T09:50:19.615085Z","shell.execute_reply":"2024-01-08T09:50:19.634564Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data5=data5.fillna(0)\n\nX = data5.drop(\"class\", axis=1)     # everything except 'class' column\ny = data5['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = None)\nclf_dtc5 = DecisionTreeClassifier()\nclf_dtc5.fit(X_train, y_train)\nfeature_cols = list(X) \nimport six\nimport sys\nsys.modules['sklearn.externals.six'] = six\n#from six imporaccuracy_scoret StringIO\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(clf_dtc5, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True, feature_names = feature_cols,class_names=['0','1','2'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('ClassifierIoT1.png')\ni=Image(graph.create_png())\ni","metadata":{"execution":{"iopub.status.busy":"2024-01-08T09:51:21.782335Z","iopub.execute_input":"2024-01-08T09:51:21.782732Z","iopub.status.idle":"2024-01-08T09:51:22.155283Z","shell.execute_reply.started":"2024-01-08T09:51:21.782701Z","shell.execute_reply":"2024-01-08T09:51:22.154369Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = clf_dtc5.predict(X_test)\nprint(y_pred)\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T09:51:27.708715Z","iopub.execute_input":"2024-01-08T09:51:27.709082Z","iopub.status.idle":"2024-01-08T09:51:27.733205Z","shell.execute_reply.started":"2024-01-08T09:51:27.709056Z","shell.execute_reply":"2024-01-08T09:51:27.732215Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save the  models\n","metadata":{}},{"cell_type":"code","source":"import pickle\n#pickle.dump(clf_dtc2, open('clf_dtc2', 'wb'))\nclf_dtc2 = pickle.load(open('clf_dtc2', 'rb'))\n#pickled_model.predict(X_test)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-09T04:31:14.771766Z","iopub.execute_input":"2024-01-09T04:31:14.772382Z","iopub.status.idle":"2024-01-09T04:31:14.780449Z","shell.execute_reply.started":"2024-01-09T04:31:14.772333Z","shell.execute_reply":"2024-01-09T04:31:14.77916Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n#pickle.dump(clf_dtc3, open('clf_dtc3', 'wb'))\nclf_dtc3 = pickle.load(open('clf_dtc3', 'rb'))\n#pickled_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T06:06:03.848256Z","iopub.execute_input":"2024-01-09T06:06:03.848834Z","iopub.status.idle":"2024-01-09T06:06:03.857072Z","shell.execute_reply.started":"2024-01-09T06:06:03.848777Z","shell.execute_reply":"2024-01-09T06:06:03.855647Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n#pickle.dump(clf_dtc4, open('clf_dtc4', 'wb'))\nclf_dtc4 = pickle.load(open('clf_dtc4', 'rb'))\n#pickled_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T04:31:23.565072Z","iopub.execute_input":"2024-01-09T04:31:23.565577Z","iopub.status.idle":"2024-01-09T04:31:23.572657Z","shell.execute_reply.started":"2024-01-09T04:31:23.565532Z","shell.execute_reply":"2024-01-09T04:31:23.571321Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n#pickle.dump(clf_dtc5, open('clf_dtc5', 'wb'))\nclf_dtc5 = pickle.load(open('clf_dtc5', 'rb'))\n#pickled_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T04:31:28.367304Z","iopub.execute_input":"2024-01-09T04:31:28.367808Z","iopub.status.idle":"2024-01-09T04:31:28.374865Z","shell.execute_reply.started":"2024-01-09T04:31:28.367768Z","shell.execute_reply":"2024-01-09T04:31:28.373562Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Now form a Ensemble Classifier ","metadata":{}},{"cell_type":"code","source":"df_data1","metadata":{"execution":{"iopub.status.busy":"2024-01-09T04:31:31.398654Z","iopub.execute_input":"2024-01-09T04:31:31.399452Z","iopub.status.idle":"2024-01-09T04:31:31.457945Z","shell.execute_reply.started":"2024-01-09T04:31:31.39941Z","shell.execute_reply":"2024-01-09T04:31:31.456683Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"orig_dataset=df_data1.drop('class',axis=1)\norig_dataset","metadata":{"execution":{"iopub.status.busy":"2024-01-08T10:12:44.888126Z","iopub.execute_input":"2024-01-08T10:12:44.888505Z","iopub.status.idle":"2024-01-08T10:12:44.931299Z","shell.execute_reply.started":"2024-01-08T10:12:44.888474Z","shell.execute_reply":"2024-01-08T10:12:44.930227Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate diffe missing values ","metadata":{}},{"cell_type":"code","source":"!pip install wget \n\nimport wget\nwget.download('https://raw.githubusercontent.com/BorisMuzellec/MissingDataOT/master/utils.py')\n\nimport numpy as np\nimport pandas as pd\nfrom utils import *\nimport torch\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:32:11.608427Z","iopub.execute_input":"2024-01-09T05:32:11.608972Z","iopub.status.idle":"2024-01-09T05:32:33.356305Z","shell.execute_reply.started":"2024-01-09T05:32:11.608936Z","shell.execute_reply":"2024-01-09T05:32:33.35481Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport missingno as msno\nhar = orig_dataset\n#har1=har.iloc[:,:-1]\nhar1=har\n#msno.heatmap(har1)\n#msno.dendrogram(har1)\nhar1=har1.values\n#print('Shape Test:\\t{}\\n'.format(har1.shape))\nhar1\n\n\nhar1=har1.astype(float)\nhar1\n","metadata":{"execution":{"iopub.status.busy":"2024-01-08T10:13:50.271645Z","iopub.execute_input":"2024-01-08T10:13:50.272457Z","iopub.status.idle":"2024-01-08T10:13:50.287152Z","shell.execute_reply.started":"2024-01-08T10:13:50.272418Z","shell.execute_reply":"2024-01-08T10:13:50.286218Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def produce_NA(har1, p_miss, mecha=\"MAR\", opt=\"logistic\", p_obs=None, q=None):\n    to_torch = torch.is_tensor(har1) ## output a pytorch tensor, or a numpy array\n    if not to_torch:\n        har1 = har1.astype(np.float32)\n        har1 = torch.from_numpy(har1)\n    \n    if mecha == \"MAR\" and opt == \"logistic\":\n        mask = MAR_mask(har1, p_miss, p_obs).double()\n        print(\"mask values\")\n        print(mask)\n    elif mecha == \"MNAR\" and opt == \"logistic\":\n        mask = MNAR_mask_logistic(har1, p_miss, p_obs).double()\n    elif mecha == \"MNAR\" and opt == \"quantile\":\n        mask = MNAR_mask_quantiles(har1, p_miss, q, 1-p_obs).double()\n    elif mecha == \"MNAR\" and opt == \"selfmasked\":\n        mask = MNAR_self_mask_logistic(har1, p_miss).double()\n    else:\n        mask = (torch.rand(har1.shape) < p_miss).double()\n     \n    \n    har1_nas = har1.clone()\n    har1_nas[mask.bool()] = np.nan\n    \n    \n    \n    \n    return {'har1_init': har1.double(), 'har1_incomp': har1_nas.double(), 'mask': mask}","metadata":{"execution":{"iopub.status.busy":"2024-01-08T10:14:09.720582Z","iopub.execute_input":"2024-01-08T10:14:09.721489Z","iopub.status.idle":"2024-01-08T10:14:09.730547Z","shell.execute_reply.started":"2024-01-08T10:14:09.721453Z","shell.execute_reply":"2024-01-08T10:14:09.72955Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"har1_miss_mar = produce_NA(har1=har1, p_miss=0.10, mecha=\"MAR\", p_obs=0.5)\n \n\n\n#print(har1_mar) \nprint(har1_miss_mar['har1_init'])\nhar1_mar = har1_miss_mar['har1_incomp']\nprint(har1_mar)  \n#har1_mar = pd.DataFrame.from_dict(har1_mar)\nR_mcar = har1_miss_mar['mask']\nprint(R_mcar)\n \n\nprint(\"Percentage of generated missing values: \", (R_mcar.sum()).numpy()/np.prod(R_mcar.size())*100, \" %\")\n\nprint(har1_mar)\nprint('Shape Test:\\t{}\\n'.format(har1_mar.shape))\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\n \nx_np = har1_mar.numpy()\nx_df = pd.DataFrame(x_np)\nx_df.to_csv('10mar_test.csv')\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\n\ntdf = pd.read_csv(\"./10mar_test.csv\").iloc[:, 1:]\ntdf.head()\n\ntdf.shape\n# Get null values and dataframe information  check for any missing data in data sets\nprint('Null Values In DataFrame: {}\\n'.format(tdf.isna().sum().sum()))\ntdf.info()\ntdf=tdf.fillna(0)\ntdf.to_csv(\"data_fil10.csv\")\n\ntdf\n\n\n\ntestdata = pd.read_csv('./10mar_test.csv').iloc[:, 1:] \n \ntestdata.head(10)\n#testdata=testdata.drop(['16'],axis=1)\ntestdata\n\nprint('Null Values In DataFrame: {}\\n'.format(testdata.isna().sum().sum()))\ntestdata.isnull().sum(0)\n\ntestdata=testdata.fillna(0)\ntestdata\n","metadata":{"execution":{"iopub.status.busy":"2024-01-08T10:15:20.008245Z","iopub.execute_input":"2024-01-08T10:15:20.009276Z","iopub.status.idle":"2024-01-08T10:15:20.253968Z","shell.execute_reply.started":"2024-01-08T10:15:20.009241Z","shell.execute_reply":"2024-01-08T10:15:20.253065Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#kfold =sklearn.model_selection.KFold(n_splits=10)\n\nimport pandas as pd\n \nfrom sklearn.metrics import log_loss\n#seed=7\n#\n\n# importing voting classifier\nfrom sklearn.ensemble import VotingClassifier\n \n    \n#Edata10Mar = supp10_data2\nEdata10Mar =testdata \nEdata10Mar\n\nX = Edata10Mar \ny= df_data1[\"class\"] \nX\nfeature_cols = list(X) \ny\n# split data\nX_train, X_test, Y_train, Y_test = train_test_split(X, y)\n\n\n\n\n\nestimators = []\nestimators.append(('c1',clf_dtc2))\nestimators.append(('c2',clf_dtc3))\nestimators.append(('c3',clf_dtc4))\nestimators.append(('c4',clf_dtc5))\n\n\n'''voting = VotingClassifier(estimators=estimators,voting='hard')\nvot_hard=voting.fit(X_train, Y_train)\n\ny_pred=vot_hard.predict(X_test)\nprint(y_pred)\nscore=accuracy_score(Y_test,y_pred)\n#print(\"hard voting score %d\" % score)\nprint(score)'''\n\n\n\n# Voting Classifier with soft voting\nvot_soft = VotingClassifier(estimators = estimators, voting ='soft')\nvot_soft.fit(X_train, Y_train)\ny_pred = vot_soft.predict(X_test)\nprint(y_pred)\n# using accuracy_score\nsscore = accuracy_score(Y_test, y_pred)\n#print(\"Soft Voting Score % d\" % score)\nprint(sscore)\n\nfrom sklearn.metrics import classification_report\n\n#print(classification_report(Y_test, y_pred))\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(Y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T10:18:30.486882Z","iopub.execute_input":"2024-01-08T10:18:30.487235Z","iopub.status.idle":"2024-01-08T10:18:30.53353Z","shell.execute_reply.started":"2024-01-08T10:18:30.48721Z","shell.execute_reply":"2024-01-08T10:18:30.532531Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **for 20% missing values**\n","metadata":{}},{"cell_type":"code","source":"har1_miss_mar = produce_NA(har1=har1, p_miss=0.20, mecha=\"MAR\", p_obs=0.5)\n \n\n\n#print(har1_mar) \nprint(har1_miss_mar['har1_init'])\nhar1_mar = har1_miss_mar['har1_incomp']\nprint(har1_mar)  \n#har1_mar = pd.DataFrame.from_dict(har1_mar)\nR_mcar = har1_miss_mar['mask']\nprint(R_mcar)\n \n\nprint(\"Percentage of generated missing values: \", (R_mcar.sum()).numpy()/np.prod(R_mcar.size())*100, \" %\")\n\nprint(har1_mar)\nprint('Shape Test:\\t{}\\n'.format(har1_mar.shape))\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\n \nx_np = har1_mar.numpy()\nx_df = pd.DataFrame(x_np)\nx_df.to_csv('10mar_test.csv')\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\n\ntdf = pd.read_csv(\"./10mar_test.csv\").iloc[:, 1:]\ntdf.head()\n\ntdf.shape\n# Get null values and dataframe information  check for any missing data in data sets\nprint('Null Values In DataFrame: {}\\n'.format(tdf.isna().sum().sum()))\ntdf.info()\ntdf=tdf.fillna(0)\ntdf.to_csv(\"data_fil10.csv\")\n\ntdf\n\n\n\ntestdata = pd.read_csv('./10mar_test.csv').iloc[:, 1:] \n \ntestdata.head(10)\n#testdata=testdata.drop(['16'],axis=1)\ntestdata\n\nprint('Null Values In DataFrame: {}\\n'.format(testdata.isna().sum().sum()))\ntestdata.isnull().sum(0)\n\ntestdata=testdata.fillna(0)\ntestdata\n\n\n\nfrom sklearn.model_selection import train_test_split\n#kfold =sklearn.model_selection.KFold(n_splits=10)\n\nimport pandas as pd\n \nfrom sklearn.metrics import log_loss\n#seed=7\n#\n\n# importing voting classifier\nfrom sklearn.ensemble import VotingClassifier\n \n    \n#Edata10Mar = supp10_data2\nEdata10Mar =testdata \nEdata10Mar\n\nX = Edata10Mar \ny= df_data1[\"class\"] \nX\nfeature_cols = list(X) \ny\n# split data\nX_train, X_test, Y_train, Y_test = train_test_split(X, y)\n\n\n\n\n\nestimators = []\nestimators.append(('c1',clf_dtc2))\nestimators.append(('c2',clf_dtc3))\nestimators.append(('c3',clf_dtc4))\nestimators.append(('c4',clf_dtc5))\n\n\n'''voting = VotingClassifier(estimators=estimators,voting='hard')\nvot_hard=voting.fit(X_train, Y_train)\n\ny_pred=vot_hard.predict(X_test)\nprint(y_pred)\nscore=accuracy_score(Y_test,y_pred)\n#print(\"hard voting score %d\" % score)\nprint(score)'''\n\n\n\n# Voting Classifier with soft voting\nvot_soft = VotingClassifier(estimators = estimators, voting ='soft')\nvot_soft.fit(X_train, Y_train)\ny_pred = vot_soft.predict(X_test)\nprint(y_pred)\n# using accuracy_score\nsscore = accuracy_score(Y_test, y_pred)\n#print(\"Soft Voting Score % d\" % score)\nprint(sscore)\n\nfrom sklearn.metrics import classification_report\n\n#print(classification_report(Y_test, y_pred))\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(Y_test, y_pred)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-08T10:25:59.205036Z","iopub.execute_input":"2024-01-08T10:25:59.205881Z","iopub.status.idle":"2024-01-08T10:25:59.320715Z","shell.execute_reply.started":"2024-01-08T10:25:59.205844Z","shell.execute_reply":"2024-01-08T10:25:59.319798Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# for 30% ","metadata":{}},{"cell_type":"code","source":"har1_miss_mar = produce_NA(har1=har1, p_miss=0.30, mecha=\"MAR\", p_obs=0.5)\n \n\n\n#print(har1_mar) \nprint(har1_miss_mar['har1_init'])\nhar1_mar = har1_miss_mar['har1_incomp']\nprint(har1_mar)  \n#har1_mar = pd.DataFrame.from_dict(har1_mar)\nR_mcar = har1_miss_mar['mask']\nprint(R_mcar)\n \n\nprint(\"Percentage of generated missing values: \", (R_mcar.sum()).numpy()/np.prod(R_mcar.size())*100, \" %\")\n\nprint(har1_mar)\nprint('Shape Test:\\t{}\\n'.format(har1_mar.shape))\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\n \nx_np = har1_mar.numpy()\nx_df = pd.DataFrame(x_np)\nx_df.to_csv('10mar_test.csv')\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\n\ntdf = pd.read_csv(\"./10mar_test.csv\").iloc[:, 1:]\ntdf.head()\n\ntdf.shape\n# Get null values and dataframe information  check for any missing data in data sets\nprint('Null Values In DataFrame: {}\\n'.format(tdf.isna().sum().sum()))\ntdf.info()\ntdf=tdf.fillna(0)\ntdf.to_csv(\"data_fil10.csv\")\n\ntdf\n\n\n\ntestdata = pd.read_csv('./10mar_test.csv').iloc[:, 1:] \n \ntestdata.head(10)\n#testdata=testdata.drop(['16'],axis=1)\ntestdata\n\nprint('Null Values In DataFrame: {}\\n'.format(testdata.isna().sum().sum()))\ntestdata.isnull().sum(0)\n\ntestdata=testdata.fillna(0)\ntestdata\n\n\n\nfrom sklearn.model_selection import train_test_split\n#kfold =sklearn.model_selection.KFold(n_splits=10)\n\nimport pandas as pd\n \nfrom sklearn.metrics import log_loss\n#seed=7\n#\n\n# importing voting classifier\nfrom sklearn.ensemble import VotingClassifier\n \n    \n#Edata10Mar = supp10_data2\nEdata10Mar =testdata \nEdata10Mar\n\nX = Edata10Mar \ny= df_data1[\"class\"] \nX\nfeature_cols = list(X) \ny\n# split data\nX_train, X_test, Y_train, Y_test = train_test_split(X, y)\n\n\n\n\n\nestimators = []\nestimators.append(('c1',clf_dtc2))\nestimators.append(('c2',clf_dtc3))\nestimators.append(('c3',clf_dtc4))\nestimators.append(('c4',clf_dtc5))\n\n\n'''voting = VotingClassifier(estimators=estimators,voting='hard')\nvot_hard=voting.fit(X_train, Y_train)\n\ny_pred=vot_hard.predict(X_test)\nprint(y_pred)\nscore=accuracy_score(Y_test,y_pred)\n#print(\"hard voting score %d\" % score)\nprint(score)'''\n\n\n\n# Voting Classifier with soft voting\nvot_soft = VotingClassifier(estimators = estimators, voting ='soft')\nvot_soft.fit(X_train, Y_train)\ny_pred = vot_soft.predict(X_test)\nprint(y_pred)\n# using accuracy_score\nsscore = accuracy_score(Y_test, y_pred)\n#print(\"Soft Voting Score % d\" % score)\nprint(sscore)\n\nfrom sklearn.metrics import classification_report\n\n#print(classification_report(Y_test, y_pred))\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(Y_test, y_pred)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-08T10:26:36.734691Z","iopub.execute_input":"2024-01-08T10:26:36.735608Z","iopub.status.idle":"2024-01-08T10:26:36.84144Z","shell.execute_reply.started":"2024-01-08T10:26:36.735575Z","shell.execute_reply":"2024-01-08T10:26:36.840393Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# For 40%","metadata":{}},{"cell_type":"code","source":"har1_miss_mar = produce_NA(har1=har1, p_miss=0.40, mecha=\"MAR\", p_obs=0.5)\n \n\n\n#print(har1_mar) \nprint(har1_miss_mar['har1_init'])\nhar1_mar = har1_miss_mar['har1_incomp']\nprint(har1_mar)  \n#har1_mar = pd.DataFrame.from_dict(har1_mar)\nR_mcar = har1_miss_mar['mask']\nprint(R_mcar)\n \n\nprint(\"Percentage of generated missing values: \", (R_mcar.sum()).numpy()/np.prod(R_mcar.size())*100, \" %\")\n\nprint(har1_mar)\nprint('Shape Test:\\t{}\\n'.format(har1_mar.shape))\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\n \nx_np = har1_mar.numpy()\nx_df = pd.DataFrame(x_np)\nx_df.to_csv('10mar_test.csv')\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\n\ntdf = pd.read_csv(\"./10mar_test.csv\").iloc[:, 1:]\ntdf.head()\n\ntdf.shape\n# Get null values and dataframe information  check for any missing data in data sets\nprint('Null Values In DataFrame: {}\\n'.format(tdf.isna().sum().sum()))\ntdf.info()\ntdf=tdf.fillna(0)\ntdf.to_csv(\"data_fil10.csv\")\n\ntdf\n\n\n\ntestdata = pd.read_csv('./10mar_test.csv').iloc[:, 1:] \n \ntestdata.head(10)\n#testdata=testdata.drop(['16'],axis=1)\ntestdata\n\nprint('Null Values In DataFrame: {}\\n'.format(testdata.isna().sum().sum()))\ntestdata.isnull().sum(0)\n\ntestdata=testdata.fillna(0)\ntestdata\n\n\n\nfrom sklearn.model_selection import train_test_split\n#kfold =sklearn.model_selection.KFold(n_splits=10)\n\nimport pandas as pd\n \nfrom sklearn.metrics import log_loss\n#seed=7\n#\n\n# importing voting classifier\nfrom sklearn.ensemble import VotingClassifier\n \n    \n#Edata10Mar = supp10_data2\nEdata10Mar =testdata \nEdata10Mar\n\nX = Edata10Mar \ny= df_data1[\"class\"] \nX\nfeature_cols = list(X) \ny\n# split data\nX_train, X_test, Y_train, Y_test = train_test_split(X, y)\n\n\n\n\n\nestimators = []\nestimators.append(('c1',clf_dtc2))\nestimators.append(('c2',clf_dtc3))\nestimators.append(('c3',clf_dtc4))\nestimators.append(('c4',clf_dtc5))\n\n\n'''voting = VotingClassifier(estimators=estimators,voting='hard')\nvot_hard=voting.fit(X_train, Y_train)\n\ny_pred=vot_hard.predict(X_test)\nprint(y_pred)\nscore=accuracy_score(Y_test,y_pred)\n#print(\"hard voting score %d\" % score)\nprint(score)'''\n\n\n\n# Voting Classifier with soft voting\nvot_soft = VotingClassifier(estimators = estimators, voting ='soft')\nvot_soft.fit(X_train, Y_train)\ny_pred = vot_soft.predict(X_test)\nprint(y_pred)\n# using accuracy_score\nsscore = accuracy_score(Y_test, y_pred)\n#print(\"Soft Voting Score % d\" % score)\nprint(sscore)\n\nfrom sklearn.metrics import classification_report\n\n#print(classification_report(Y_test, y_pred))\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(Y_test, y_pred)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-08T10:27:04.802108Z","iopub.execute_input":"2024-01-08T10:27:04.802513Z","iopub.status.idle":"2024-01-08T10:27:04.909617Z","shell.execute_reply.started":"2024-01-08T10:27:04.802481Z","shell.execute_reply":"2024-01-08T10:27:04.908728Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# For 50%","metadata":{}},{"cell_type":"code","source":"har1_miss_mar = produce_NA(har1=har1, p_miss=0.50, mecha=\"MAR\", p_obs=0.5)\n \n\n\n#print(har1_mar) \nprint(har1_miss_mar['har1_init'])\nhar1_mar = har1_miss_mar['har1_incomp']\nprint(har1_mar)  \n#har1_mar = pd.DataFrame.from_dict(har1_mar)\nR_mcar = har1_miss_mar['mask']\nprint(R_mcar)\n \n\nprint(\"Percentage of generated missing values: \", (R_mcar.sum()).numpy()/np.prod(R_mcar.size())*100, \" %\")\n\nprint(har1_mar)\nprint('Shape Test:\\t{}\\n'.format(har1_mar.shape))\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\n \nx_np = har1_mar.numpy()\nx_df = pd.DataFrame(x_np)\nx_df.to_csv('10mar_test.csv')\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\n\ntdf = pd.read_csv(\"./10mar_test.csv\").iloc[:, 1:]\ntdf.head()\n\ntdf.shape\n# Get null values and dataframe information  check for any missing data in data sets\nprint('Null Values In DataFrame: {}\\n'.format(tdf.isna().sum().sum()))\ntdf.info()\ntdf=tdf.fillna(0)\ntdf.to_csv(\"data_fil10.csv\")\n\ntdf\n\n\n\ntestdata = pd.read_csv('./10mar_test.csv').iloc[:, 1:] \n \ntestdata.head(10)\n#testdata=testdata.drop(['16'],axis=1)\ntestdata\n\nprint('Null Values In DataFrame: {}\\n'.format(testdata.isna().sum().sum()))\ntestdata.isnull().sum(0)\n\ntestdata=testdata.fillna(0)\ntestdata\n\n\n\nfrom sklearn.model_selection import train_test_split\n#kfold =sklearn.model_selection.KFold(n_splits=10)\n\nimport pandas as pd\n \nfrom sklearn.metrics import log_loss\n#seed=7\n#\n\n# importing voting classifier\nfrom sklearn.ensemble import VotingClassifier\n \n    \n#Edata10Mar = supp10_data2\nEdata10Mar =testdata \nEdata10Mar\n\nX = Edata10Mar \ny= df_data1[\"class\"] \nX\nfeature_cols = list(X) \ny\n# split data\nX_train, X_test, Y_train, Y_test = train_test_split(X, y)\n\n\n\n\n\nestimators = []\nestimators.append(('c1',clf_dtc2))\nestimators.append(('c2',clf_dtc3))\nestimators.append(('c3',clf_dtc4))\nestimators.append(('c4',clf_dtc5))\n\n\n'''voting = VotingClassifier(estimators=estimators,voting='hard')\nvot_hard=voting.fit(X_train, Y_train)\n\ny_pred=vot_hard.predict(X_test)\nprint(y_pred)\nscore=accuracy_score(Y_test,y_pred)\n#print(\"hard voting score %d\" % score)\nprint(score)'''\n\n\n\n# Voting Classifier with soft voting\nvot_soft = VotingClassifier(estimators = estimators, voting ='soft')\nvot_soft.fit(X_train, Y_train)\ny_pred = vot_soft.predict(X_test)\nprint(y_pred)\n# using accuracy_score\nsscore = accuracy_score(Y_test, y_pred)\n#print(\"Soft Voting Score % d\" % score)\nprint(sscore)\n\nfrom sklearn.metrics import classification_report\n\n#print(classification_report(Y_test, y_pred))\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(Y_test, y_pred)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-08T10:27:25.908025Z","iopub.execute_input":"2024-01-08T10:27:25.908384Z","iopub.status.idle":"2024-01-08T10:27:26.007085Z","shell.execute_reply.started":"2024-01-08T10:27:25.908358Z","shell.execute_reply":"2024-01-08T10:27:26.006111Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluate the DT performance on 10 to 50% of Missing Values \n","metadata":{}},{"cell_type":"code","source":"orig_dataset=df_data1.drop('class',axis=1)\norig_dataset\nimport pandas as pd\nimport missingno as msno\nhar = orig_dataset\n#har1=har.iloc[:,:-1]\nhar1=har\n#msno.heatmap(har1)\n#msno.dendrogram(har1)\nhar1=har1.values\n#print('Shape Test:\\t{}\\n'.format(har1.shape))\nhar1\n\n\nhar1=har1.astype(float)\nhar1\n\nhar1_miss_mar = produce_NA(har1=har1, p_miss=0.50, mecha=\"MAR\", p_obs=0.5)\n \n\n\n#print(har1_mar) \nprint(har1_miss_mar['har1_init'])\nhar1_mar = har1_miss_mar['har1_incomp']\nprint(har1_mar)  \n#har1_mar = pd.DataFrame.from_dict(har1_mar)\nR_mcar = har1_miss_mar['mask']\nprint(R_mcar)\n \n\nprint(\"Percentage of generated missing values: \", (R_mcar.sum()).numpy()/np.prod(R_mcar.size())*100, \" %\")\n\nprint(har1_mar)\nprint('Shape Test:\\t{}\\n'.format(har1_mar.shape))\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\n \nx_np = har1_mar.numpy()\nx_df = pd.DataFrame(x_np)\nx_df.to_csv('10mar_test.csv')\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\n\ntdf = pd.read_csv(\"./10mar_test.csv\").iloc[:, 1:]\ntdf.head()\n\ntdf.shape\n# Get null values and dataframe information  check for any missing data in data sets\nprint('Null Values In DataFrame: {}\\n'.format(tdf.isna().sum().sum()))\ntdf.info()\ntdf=tdf.fillna(0)\ntdf.to_csv(\"data_fil10.csv\")\n\ntdf\n\n\n\ntestdata = pd.read_csv('./10mar_test.csv').iloc[:, 1:] \n \ntestdata.head(10)\n#testdata=testdata.drop(['16'],axis=1)\ntestdata\n\nprint('Null Values In DataFrame: {}\\n'.format(testdata.isna().sum().sum()))\ntestdata.isnull().sum(0)\n\ntestdata=testdata.fillna(0)\ntestdata\nfrom sklearn.model_selection import train_test_split\n#kfold =sklearn.model_selection.KFold(n_splits=10)\n\nimport pandas as pd\n \nfrom sklearn.metrics import log_loss\n#seed=7\n#\n\n# importing voting classifier\nfrom sklearn.ensemble import VotingClassifier\n \n    \n#Edata10Mar = supp10_data2\nEdata10Mar =testdata \nEdata10Mar\nX = Edata10Mar  \ny= df_data1[\"class\"] \nX\nfeature_cols = list(X) \ny\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = None)\nclf_dtc5 = DecisionTreeClassifier()\nclf_dtc5.fit(X_train, y_train)\nfeature_cols = list(X) \nimport six\nimport sys\nsys.modules['sklearn.externals.six'] = six\n#from six imporaccuracy_scoret StringIO\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(clf_dtc5, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True, feature_names = feature_cols,class_names=['0','1','2'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('ClassifierIoT1.png')\ni=Image(graph.create_png())\ni\n\n\n\ny_pred = clf_dtc5.predict(X_test)\nprint(y_pred)\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T10:38:58.051399Z","iopub.execute_input":"2024-01-08T10:38:58.051794Z","iopub.status.idle":"2024-01-08T10:38:58.721965Z","shell.execute_reply.started":"2024-01-08T10:38:58.051761Z","shell.execute_reply":"2024-01-08T10:38:58.72099Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**","metadata":{}},{"cell_type":"markdown","source":"# Other Classifiers - Performance ","metadata":{}},{"cell_type":"markdown","source":"# 1. SVC","metadata":{}},{"cell_type":"code","source":" ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"orig_dataset=df_data1.drop('class',axis=1)\norig_dataset\nimport pandas as pd\nimport missingno as msno\nhar = orig_dataset\n#har1=har.iloc[:,:-1]\nhar1=har\n#msno.heatmap(har1)\n#msno.dendrogram(har1)\nhar1=har1.values\n#print('Shape Test:\\t{}\\n'.format(har1.shape))\nhar1\n\n\nhar1=har1.astype(float)\nhar1\n\nhar1_miss_mar = produce_NA(har1=har1, p_miss=0.50, mecha=\"MAR\", p_obs=0.5)\n \n\n\n#print(har1_mar) \nprint(har1_miss_mar['har1_init'])\nhar1_mar = har1_miss_mar['har1_incomp']\nprint(har1_mar)  \n#har1_mar = pd.DataFrame.from_dict(har1_mar)\nR_mcar = har1_miss_mar['mask']\nprint(R_mcar)\n \n\nprint(\"Percentage of generated missing values: \", (R_mcar.sum()).numpy()/np.prod(R_mcar.size())*100, \" %\")\n\nprint(har1_mar)\nprint('Shape Test:\\t{}\\n'.format(har1_mar.shape))\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\n \nx_np = har1_mar.numpy()\nx_df = pd.DataFrame(x_np)\nx_df.to_csv('10mar_test.csv')\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\n\ntdf = pd.read_csv(\"./10mar_test.csv\").iloc[:, 1:]\ntdf.head()\n\ntdf.shape\n# Get null values and dataframe information  check for any missing data in data sets\nprint('Null Values In DataFrame: {}\\n'.format(tdf.isna().sum().sum()))\ntdf.info()\ntdf=tdf.fillna(0)\ntdf.to_csv(\"data_fil10.csv\")\n\ntdf\n\n\n\ntestdata = pd.read_csv('./10mar_test.csv').iloc[:, 1:] \n \ntestdata.head(10)\n#testdata=testdata.drop(['16'],axis=1)\ntestdata\n\nprint('Null Values In DataFrame: {}\\n'.format(testdata.isna().sum().sum()))\ntestdata.isnull().sum(0)\n\ntestdata=testdata.fillna(0)\ntestdata\nfrom sklearn.model_selection import train_test_split\n#kfold =sklearn.model_selection.KFold(n_splits=10)\n\nimport pandas as pd\n \nfrom sklearn.metrics import log_loss\n#seed=7\n#\n\n# importing voting classifier\nfrom sklearn.ensemble import VotingClassifier\n \n    \n#Edata10Mar = supp10_data2\nEdata10Mar =testdata \nEdata10Mar\nX = Edata10Mar  \ny= df_data1[\"class\"] \nX\nfeature_cols = list(X) \ny\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = None)\n\nclf_dtc5 = svm.SVC(kernel='linear')\nclf_dtc5.fit(X_train, y_train)\n\n#clf_dtc5 = DecisionTreeClassifier()\n#clf_dtc5.fit(X_train, y_train)\n\nfeature_cols = list(X) \n\n\n\n\ny_pred = clf_dtc5.predict(X_test)\nprint(y_pred)\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T10:56:19.366419Z","iopub.execute_input":"2024-01-08T10:56:19.36687Z","iopub.status.idle":"2024-01-08T10:56:28.564615Z","shell.execute_reply.started":"2024-01-08T10:56:19.366838Z","shell.execute_reply":"2024-01-08T10:56:28.563598Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# KNN -Classifier ","metadata":{}},{"cell_type":"code","source":"orig_dataset=df_data1.drop('class',axis=1)\norig_dataset\nimport pandas as pd\nimport missingno as msno\nhar = orig_dataset\n#har1=har.iloc[:,:-1]\nhar1=har\n#msno.heatmap(har1)\n#msno.dendrogram(har1)\nhar1=har1.values\n#print('Shape Test:\\t{}\\n'.format(har1.shape))\nhar1\n\n\nhar1=har1.astype(float)\nhar1\n\nhar1_miss_mar = produce_NA(har1=har1, p_miss=0.50, mecha=\"MAR\", p_obs=0.5)\n \n\n\n#print(har1_mar) \nprint(har1_miss_mar['har1_init'])\nhar1_mar = har1_miss_mar['har1_incomp']\nprint(har1_mar)  \n#har1_mar = pd.DataFrame.from_dict(har1_mar)\nR_mcar = har1_miss_mar['mask']\nprint(R_mcar)\n \n\nprint(\"Percentage of generated missing values: \", (R_mcar.sum()).numpy()/np.prod(R_mcar.size())*100, \" %\")\n\nprint(har1_mar)\nprint('Shape Test:\\t{}\\n'.format(har1_mar.shape))\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\n \nx_np = har1_mar.numpy()\nx_df = pd.DataFrame(x_np)\nx_df.to_csv('10mar_test.csv')\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\n\ntdf = pd.read_csv(\"./10mar_test.csv\").iloc[:, 1:]\ntdf.head()\n\ntdf.shape\n# Get null values and dataframe information  check for any missing data in data sets\nprint('Null Values In DataFrame: {}\\n'.format(tdf.isna().sum().sum()))\ntdf.info()\ntdf=tdf.fillna(0)\ntdf.to_csv(\"data_fil10.csv\")\n\ntdf\n\n\n\ntestdata = pd.read_csv('./10mar_test.csv').iloc[:, 1:] \n \ntestdata.head(10)\n#testdata=testdata.drop(['16'],axis=1)\ntestdata\n\nprint('Null Values In DataFrame: {}\\n'.format(testdata.isna().sum().sum()))\ntestdata.isnull().sum(0)\n\ntestdata=testdata.fillna(0)\ntestdata\nfrom sklearn.model_selection import train_test_split\n#kfold =sklearn.model_selection.KFold(n_splits=10)\n\nimport pandas as pd\n \nfrom sklearn.metrics import log_loss\n#seed=7\n#\n\n# importing voting classifier\nfrom sklearn.ensemble import VotingClassifier\n \n    \n#Edata10Mar = supp10_data2\nEdata10Mar =testdata \nEdata10Mar\nX = Edata10Mar  \ny= df_data1[\"class\"] \nX\nfeature_cols = list(X) \ny\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = None)\n\nclf_dtc5 = KNeighborsClassifier(n_neighbors=5)\nclf_dtc5.fit(X_train, y_train)\n\n#clf_dtc5 = DecisionTreeClassifier()\n#clf_dtc5.fit(X_train, y_train)\n\nfeature_cols = list(X) \n\n\n\n\ny_pred = clf_dtc5.predict(X_test)\nprint(y_pred)\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T11:02:42.289245Z","iopub.execute_input":"2024-01-08T11:02:42.289624Z","iopub.status.idle":"2024-01-08T11:02:42.378992Z","shell.execute_reply.started":"2024-01-08T11:02:42.289593Z","shell.execute_reply":"2024-01-08T11:02:42.377969Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Random Forest ","metadata":{}},{"cell_type":"code","source":"orig_dataset=df_data1.drop('class',axis=1)\norig_dataset\nimport pandas as pd\nimport missingno as msno\nhar = orig_dataset\n#har1=har.iloc[:,:-1]\nhar1=har\n#msno.heatmap(har1)\n#msno.dendrogram(har1)\nhar1=har1.values\n#print('Shape Test:\\t{}\\n'.format(har1.shape))\nhar1\n\n\nhar1=har1.astype(float)\nhar1\n\nhar1_miss_mar = produce_NA(har1=har1, p_miss=0.10, mecha=\"MAR\", p_obs=0.5)\n \n\n\n#print(har1_mar) \nprint(har1_miss_mar['har1_init'])\nhar1_mar = har1_miss_mar['har1_incomp']\nprint(har1_mar)  \n#har1_mar = pd.DataFrame.from_dict(har1_mar)\nR_mcar = har1_miss_mar['mask']\nprint(R_mcar)\n \n\nprint(\"Percentage of generated missing values: \", (R_mcar.sum()).numpy()/np.prod(R_mcar.size())*100, \" %\")\n\nprint(har1_mar)\nprint('Shape Test:\\t{}\\n'.format(har1_mar.shape))\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\n \nx_np = har1_mar.numpy()\nx_df = pd.DataFrame(x_np)\nx_df.to_csv('10mar_test.csv')\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\n\ntdf = pd.read_csv(\"./10mar_test.csv\").iloc[:, 1:]\ntdf.head()\n\ntdf.shape\n# Get null values and dataframe information  check for any missing data in data sets\nprint('Null Values In DataFrame: {}\\n'.format(tdf.isna().sum().sum()))\ntdf.info()\ntdf=tdf.fillna(0)\ntdf.to_csv(\"data_fil10.csv\")\n\ntdf\n\n\n\ntestdata = pd.read_csv('./10mar_test.csv').iloc[:, 1:] \n \ntestdata.head(10)\n#testdata=testdata.drop(['16'],axis=1)\ntestdata\n\nprint('Null Values In DataFrame: {}\\n'.format(testdata.isna().sum().sum()))\ntestdata.isnull().sum(0)\n\ntestdata=testdata.fillna(0)\ntestdata\nfrom sklearn.model_selection import train_test_split\n#kfold =sklearn.model_selection.KFold(n_splits=10)\n\nimport pandas as pd\n \nfrom sklearn.metrics import log_loss\n#seed=7\n#\n\n# importing voting classifier\nfrom sklearn.ensemble import VotingClassifier\n \n    \n#Edata10Mar = supp10_data2\nEdata10Mar =testdata \nEdata10Mar\nX = Edata10Mar  \ny= df_data1[\"class\"] \nX\nfeature_cols = list(X) \ny\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = None)\n\nclf_dtc5 = RandomForestClassifier()\nclf_dtc5.fit(X_train, y_train)\n\n#clf_dtc5 = DecisionTreeClassifier()\n#clf_dtc5.fit(X_train, y_train)\n\nfeature_cols = list(X) \n\n\n\n\ny_pred = clf_dtc5.predict(X_test)\nprint(y_pred)\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T11:08:08.231676Z","iopub.execute_input":"2024-01-08T11:08:08.232061Z","iopub.status.idle":"2024-01-08T11:08:08.514635Z","shell.execute_reply.started":"2024-01-08T11:08:08.232031Z","shell.execute_reply":"2024-01-08T11:08:08.513657Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create aSecond Balanced Dataset for exploring the same \n","metadata":{}},{"cell_type":"code","source":"df_datab2=pd.read_csv(\"/kaggle/input/b2-dataset/B2-newmodifieddataset.csv\").iloc[:, 1:]\n\ndf_datab2=df_datab2.fillna(0)\n\ndf_datab2.info()\n\n\nX = df_datab2.drop(\"class\", axis=1)     # everything except 'class' column\ny = df_datab2['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = None)\nclf_dtcb21 = DecisionTreeClassifier()\nclf_dtcb21.fit(X_train, y_train)\nfeature_cols = list(X) \nimport six\nimport sys\nsys.modules['sklearn.externals.six'] = six\n#from six imporaccuracy_scoret StringIO\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(clf_dtcb21, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True, feature_names = feature_cols,class_names=['0','1','2'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('ClassifierIoTb21.png')\ni=Image(graph.create_png())\ni\n\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:02:55.977141Z","iopub.execute_input":"2024-01-09T05:02:55.977882Z","iopub.status.idle":"2024-01-09T05:02:56.418517Z","shell.execute_reply.started":"2024-01-09T05:02:55.977822Z","shell.execute_reply":"2024-01-09T05:02:56.41695Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = clf_dtcb21.predict(X_test)\nprint(y_pred)\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:03:02.124046Z","iopub.execute_input":"2024-01-09T05:03:02.1245Z","iopub.status.idle":"2024-01-09T05:03:02.155594Z","shell.execute_reply.started":"2024-01-09T05:03:02.124467Z","shell.execute_reply":"2024-01-09T05:03:02.154028Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create a modified Dataset ","metadata":{}},{"cell_type":"code","source":"columns = ['age','blood_pressure','specific_gravity','albumin','sugar',' red_blood_cells','pus_cell','pus_cell_clumps','bacteria','blood_urea',' serum_creatinine ','potassium ','white_blood_cell_count','red_blood_cell_count ','hypertension','diabetes_mellitus ','coronary_artery_disease',' appetite ','anemia','class']\ndatab21 = pd.DataFrame(df_datab2, columns=columns)\ndatab21\ndatab21=datab21.fillna(0)\n\nX = datab21.drop(\"class\", axis=1)     # everything except 'class' column\ny = datab21['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = None)\nclf_dtcb22 = DecisionTreeClassifier()\nclf_dtcb22.fit(X_train, y_train)\nfeature_cols = list(X) \nimport six\nimport sys\nsys.modules['sklearn.externals.six'] = six\n#from six imporaccuracy_scoret StringIO\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(clf_dtcb22, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True, feature_names = feature_cols,class_names=['0','1','2'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('ClassifierIoTb22.png')\ni=Image(graph.create_png())\ni","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:12:34.688941Z","iopub.execute_input":"2024-01-09T05:12:34.689517Z","iopub.status.idle":"2024-01-09T05:12:35.199661Z","shell.execute_reply.started":"2024-01-09T05:12:34.689481Z","shell.execute_reply":"2024-01-09T05:12:35.198197Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ny_pred = clf_dtcb22.predict(X_test)\nprint(y_pred)\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:12:41.026096Z","iopub.execute_input":"2024-01-09T05:12:41.026584Z","iopub.status.idle":"2024-01-09T05:12:41.057864Z","shell.execute_reply.started":"2024-01-09T05:12:41.026549Z","shell.execute_reply":"2024-01-09T05:12:41.056647Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modified Dataset ","metadata":{}},{"cell_type":"code","source":"columns = ['sugar',' red_blood_cells','pus_cell','pus_cell_clumps','bacteria','blood_urea',' serum_creatinine ','potassium ','red_blood_cell_count ','diabetes_mellitus ','coronary_artery_disease',' appetite ','anemia','class']\ndatab22 = pd.DataFrame(df_datab2, columns=columns)\ndatab22\ndatab22=datab22.fillna(0)\n\nX = datab22.drop(\"class\", axis=1)     # everything except 'class' column\ny = datab22['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = None)\nclf_dtcb23 = DecisionTreeClassifier()\nclf_dtcb23.fit(X_train, y_train)\nfeature_cols = list(X) \nimport six\nimport sys\nsys.modules['sklearn.externals.six'] = six\n#from six imporaccuracy_scoret StringIO\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(clf_dtcb23, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True, feature_names = feature_cols,class_names=['0','1','2'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('ClassifierIoTb23.png')\ni=Image(graph.create_png())\ni\n\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:16:44.276307Z","iopub.execute_input":"2024-01-09T05:16:44.276868Z","iopub.status.idle":"2024-01-09T05:16:46.323426Z","shell.execute_reply.started":"2024-01-09T05:16:44.276829Z","shell.execute_reply":"2024-01-09T05:16:46.322079Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = clf_dtcb23.predict(X_test)\nprint(y_pred)\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:16:52.034226Z","iopub.execute_input":"2024-01-09T05:16:52.034668Z","iopub.status.idle":"2024-01-09T05:16:52.065054Z","shell.execute_reply.started":"2024-01-09T05:16:52.034636Z","shell.execute_reply":"2024-01-09T05:16:52.063792Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modified Dataset \n","metadata":{}},{"cell_type":"code","source":"columns = [' red_blood_cells','bacteria',' serum_creatinine ',\n           'potassium ','red_blood_cell_count ','diabetes_mellitus ',\n           'coronary_artery_disease',' appetite ','class']\ndatab23 = pd.DataFrame(df_datab2, columns=columns)\ndatab23\ndatab23=datab23.fillna(0)\n\nX = datab23.drop(\"class\", axis=1)     # everything except 'class' column\ny = datab23['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = None)\nclf_dtcb24 = DecisionTreeClassifier()\nclf_dtcb24.fit(X_train, y_train)\nfeature_cols = list(X) \nimport six\nimport sys\nsys.modules['sklearn.externals.six'] = six\n#from six imporaccuracy_scoret StringIO\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(clf_dtcb24, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True, feature_names = feature_cols,class_names=['0','1','2'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('ClassifierIoTb23.png')\ni=Image(graph.create_png())\ni","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:23:27.787385Z","iopub.execute_input":"2024-01-09T05:23:27.787965Z","iopub.status.idle":"2024-01-09T05:23:28.018476Z","shell.execute_reply.started":"2024-01-09T05:23:27.787928Z","shell.execute_reply":"2024-01-09T05:23:28.017022Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = clf_dtcb24.predict(X_test)\nprint(y_pred)\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(y_test, y_pred)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:23:32.809088Z","iopub.execute_input":"2024-01-09T05:23:32.809552Z","iopub.status.idle":"2024-01-09T05:23:32.84142Z","shell.execute_reply.started":"2024-01-09T05:23:32.80952Z","shell.execute_reply":"2024-01-09T05:23:32.839572Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modifed dataset \n","metadata":{}},{"cell_type":"code","source":"columns = ['red_blood_cells',' serum_creatinine ','potassium','red_blood_cell_count ','diabetes_mellitus ', 'appetite ','class']\ndatab24 = pd.DataFrame(df_datab2, columns=columns)\ndatab24\ndatab24=datab24.fillna(0)\n\nX = datab24.drop(\"class\", axis=1)     # everything except 'class' column\ny = datab24['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = None)\nclf_dtcb25 = DecisionTreeClassifier()\nclf_dtcb25.fit(X_train, y_train)\nfeature_cols = list(X) \nimport six\nimport sys\nsys.modules['sklearn.externals.six'] = six\n#from six imporaccuracy_scoret StringIO\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(clf_dtcb25, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True, feature_names = feature_cols,class_names=['0','1','2'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('ClassifierIoTb23.png')\ni=Image(graph.create_png())\ni\n","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:26:05.058528Z","iopub.execute_input":"2024-01-09T05:26:05.059567Z","iopub.status.idle":"2024-01-09T05:26:06.463368Z","shell.execute_reply.started":"2024-01-09T05:26:05.059524Z","shell.execute_reply":"2024-01-09T05:26:06.462085Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = clf_dtcb25.predict(X_test)\nprint(y_pred)\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:26:20.3421Z","iopub.execute_input":"2024-01-09T05:26:20.342612Z","iopub.status.idle":"2024-01-09T05:26:20.373259Z","shell.execute_reply.started":"2024-01-09T05:26:20.342575Z","shell.execute_reply":"2024-01-09T05:26:20.371736Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modified Dataset ","metadata":{}},{"cell_type":"code","source":"columns = ['serum_creatinine ','red_blood_cell_count ','diabetes_mellitus ','appetite ','class']\ndatab25 = pd.DataFrame(df_datab2, columns=columns)\ndatab25\ndatab25=datab25.fillna(0)\n\nX = datab25.drop(\"class\", axis=1)     # everything except 'class' column\ny = datab25['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = None)\nclf_dtcb26 = DecisionTreeClassifier()\nclf_dtcb26.fit(X_train, y_train)\nfeature_cols = list(X) \nimport six\nimport sys\nsys.modules['sklearn.externals.six'] = six\n#from six imporaccuracy_scoret StringIO\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(clf_dtcb26, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True, feature_names = feature_cols,class_names=['0','1','2'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('ClassifierIoTb26.png')\ni=Image(graph.create_png())\ni\n","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:29:56.346559Z","iopub.execute_input":"2024-01-09T05:29:56.347026Z","iopub.status.idle":"2024-01-09T05:29:56.452842Z","shell.execute_reply.started":"2024-01-09T05:29:56.346992Z","shell.execute_reply":"2024-01-09T05:29:56.451533Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As no attributes are considered for DT construction we neglect thme ","metadata":{}},{"cell_type":"markdown","source":"# Save B2 Subspace classifiers ","metadata":{}},{"cell_type":"code","source":"import pickle\npickle.dump(clf_dtcb21, open('clf_dtcb21', 'wb'))\nclf_dtcb21 = pickle.load(open('clf_dtcb21', 'rb'))\n#pickled_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:35:18.470483Z","iopub.execute_input":"2024-01-09T05:35:18.471797Z","iopub.status.idle":"2024-01-09T05:35:18.481837Z","shell.execute_reply.started":"2024-01-09T05:35:18.471746Z","shell.execute_reply":"2024-01-09T05:35:18.480296Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\npickle.dump(clf_dtcb22, open('clf_dtcb22', 'wb'))\nclf_dtcb22 = pickle.load(open('clf_dtcb22', 'rb'))\n#pickled_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:35:39.063415Z","iopub.execute_input":"2024-01-09T05:35:39.06388Z","iopub.status.idle":"2024-01-09T05:35:39.071366Z","shell.execute_reply.started":"2024-01-09T05:35:39.063847Z","shell.execute_reply":"2024-01-09T05:35:39.070188Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\npickle.dump(clf_dtcb23, open('clf_dtcb23', 'wb'))\nclf_dtcb23 = pickle.load(open('clf_dtcb23', 'rb'))\n#pickled_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:35:52.717608Z","iopub.execute_input":"2024-01-09T05:35:52.718407Z","iopub.status.idle":"2024-01-09T05:35:52.7298Z","shell.execute_reply.started":"2024-01-09T05:35:52.718358Z","shell.execute_reply":"2024-01-09T05:35:52.727106Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\npickle.dump(clf_dtcb24, open('clf_dtcb24', 'wb'))\nclf_dtcb24 = pickle.load(open('clf_dtcb24', 'rb'))\n#pickled_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:36:06.117237Z","iopub.execute_input":"2024-01-09T05:36:06.11779Z","iopub.status.idle":"2024-01-09T05:36:06.128005Z","shell.execute_reply.started":"2024-01-09T05:36:06.117747Z","shell.execute_reply":"2024-01-09T05:36:06.126347Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\npickle.dump(clf_dtcb25, open('clf_dtcb25', 'wb'))\nclf_dtcb25 = pickle.load(open('clf_dtcb25', 'rb'))\n#pickled_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:36:21.454599Z","iopub.execute_input":"2024-01-09T05:36:21.455266Z","iopub.status.idle":"2024-01-09T05:36:21.464102Z","shell.execute_reply.started":"2024-01-09T05:36:21.455225Z","shell.execute_reply":"2024-01-09T05:36:21.4625Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Now create a Ensemble classifier for B2 dataset  and test with different percentages of missing values ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport missingno as msno\n\ndf_datab2=pd.read_csv(\"/kaggle/input/b2-dataset/B2-newmodifieddataset.csv\").iloc[:, 1:]\n\ndf_datab2=df_datab2.fillna(0)\n\ndf_datab2.info()\n\n\nhar = df_datab2\n#har1=har.iloc[:,:-1]\nhar1=har\n#msno.heatmap(har1)\n#msno.dendrogram(har1)\nhar1=har1.values\n#print('Shape Test:\\t{}\\n'.format(har1.shape))\nhar1\n\n\nhar1=har1.astype(float)\nhar1\n\n\ndef produce_NA(har1, p_miss, mecha=\"MAR\", opt=\"logistic\", p_obs=None, q=None):\n    to_torch = torch.is_tensor(har1) ## output a pytorch tensor, or a numpy array\n    if not to_torch:\n        har1 = har1.astype(np.float32)\n        har1 = torch.from_numpy(har1)\n    \n    if mecha == \"MAR\" and opt == \"logistic\":\n        mask = MAR_mask(har1, p_miss, p_obs).double()\n        print(\"mask values\")\n        print(mask)\n    elif mecha == \"MNAR\" and opt == \"logistic\":\n        mask = MNAR_mask_logistic(har1, p_miss, p_obs).double()\n    elif mecha == \"MNAR\" and opt == \"quantile\":\n        mask = MNAR_mask_quantiles(har1, p_miss, q, 1-p_obs).double()\n    elif mecha == \"MNAR\" and opt == \"selfmasked\":\n        mask = MNAR_self_mask_logistic(har1, p_miss).double()\n    else:\n        mask = (torch.rand(har1.shape) < p_miss).double()\n     \n    \n    har1_nas = har1.clone()\n    har1_nas[mask.bool()] = np.nan\n    \n    \n    \n    \n    return {'har1_init': har1.double(), 'har1_incomp': har1_nas.double(), 'mask': mask}\n\n\n\nhar1_miss_mar = produce_NA(har1=har1, p_miss=0.10, mecha=\"MAR\", p_obs=0.5)\n \n\n\n#print(har1_mar) \nprint(har1_miss_mar['har1_init'])\nhar1_mar = har1_miss_mar['har1_incomp']\nprint(har1_mar)  \n#har1_mar = pd.DataFrame.from_dict(har1_mar)\nR_mcar = har1_miss_mar['mask']\nprint(R_mcar)\n \n\nprint(\"Percentage of generated missing values: \", (R_mcar.sum()).numpy()/np.prod(R_mcar.size())*100, \" %\")\n\nprint(har1_mar)\nprint('Shape Test:\\t{}\\n'.format(har1_mar.shape))\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\n \nx_np = har1_mar.numpy()\nx_df = pd.DataFrame(x_np)\nx_df.to_csv('10mar_test.csv')\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\n\ntdf = pd.read_csv(\"./10mar_test.csv\").iloc[:, 1:]\ntdf.head()\n\ntdf.shape\n# Get null values and dataframe information  check for any missing data in data sets\nprint('Null Values In DataFrame: {}\\n'.format(tdf.isna().sum().sum()))\ntdf.info()\ntdf=tdf.fillna(0)\ntdf.to_csv(\"data_fil100.csv\")\n\ntdf\n\n\n\ntestdata = pd.read_csv('./10mar_test.csv').iloc[:, 1:] \n \ntestdata.head(10)\n#testdata=testdata.drop(['16'],axis=1)\ntestdata\n\nprint('Null Values In DataFrame: {}\\n'.format(testdata.isna().sum().sum()))\ntestdata.isnull().sum(0)\n\ntestdata=testdata.fillna(0)\ntestdata\n\n\n\nfrom sklearn.model_selection import train_test_split\n#kfold =sklearn.model_selection.KFold(n_splits=10)\n\nimport pandas as pd\n \nfrom sklearn.metrics import log_loss\n#seed=7\n#\n\n# importing voting classifier\nfrom sklearn.ensemble import VotingClassifier\n \n    \n#Edata10Mar = supp10_data2\nEdata10Mar =testdata \nEdata10Mar\n\nX = Edata10Mar \ny= df_datab2[\"class\"] \nX\nfeature_cols = list(X) \ny\n# split data\nX_train, X_test, Y_train, Y_test = train_test_split(X, y)\n\n\n\n\n\nestimators = []\nestimators.append(('c1',clf_dtcb21))\nestimators.append(('c2',clf_dtcb22))\nestimators.append(('c3',clf_dtcb23))\nestimators.append(('c4',clf_dtcb24))\nestimators.append(('c5',clf_dtcb25))\n\n\n\n'''voting = VotingClassifier(estimators=estimators,voting='hard')\nvot_hard=voting.fit(X_train, Y_train)\n\ny_pred=vot_hard.predict(X_test)\nprint(y_pred)\nscore=accuracy_score(Y_test,y_pred)\n#print(\"hard voting score %d\" % score)\nprint(score)'''\n\n\n\n# Voting Classifier with soft voting\nvot_soft = VotingClassifier(estimators = estimators, voting ='soft')\nvot_soft.fit(X_train, Y_train)\ny_pred = vot_soft.predict(X_test)\nprint(y_pred)\n# using accuracy_score\nsscore = accuracy_score(Y_test, y_pred)\n#print(\"Soft Voting Score % d\" % score)\nprint(sscore)\n\nfrom sklearn.metrics import classification_report\n\n#print(classification_report(Y_test, y_pred))\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(Y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:55:25.109796Z","iopub.execute_input":"2024-01-09T05:55:25.110488Z","iopub.status.idle":"2024-01-09T05:55:25.288148Z","shell.execute_reply.started":"2024-01-09T05:55:25.110436Z","shell.execute_reply":"2024-01-09T05:55:25.286613Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport missingno as msno\n\ndf_datab2=pd.read_csv(\"/kaggle/input/b2-dataset/B2-newmodifieddataset.csv\").iloc[:, 1:]\n\ndf_datab2=df_datab2.fillna(0)\n\ndf_datab2.info()\n\n\nhar = df_datab2\n#har1=har.iloc[:,:-1]\nhar1=har\n#msno.heatmap(har1)\n#msno.dendrogram(har1)\nhar1=har1.values\n#print('Shape Test:\\t{}\\n'.format(har1.shape))\nhar1\n\n\nhar1=har1.astype(float)\nhar1\n\n\ndef produce_NA(har1, p_miss, mecha=\"MAR\", opt=\"logistic\", p_obs=None, q=None):\n    to_torch = torch.is_tensor(har1) ## output a pytorch tensor, or a numpy array\n    if not to_torch:\n        har1 = har1.astype(np.float32)\n        har1 = torch.from_numpy(har1)\n    \n    if mecha == \"MAR\" and opt == \"logistic\":\n        mask = MAR_mask(har1, p_miss, p_obs).double()\n        print(\"mask values\")\n        print(mask)\n    elif mecha == \"MNAR\" and opt == \"logistic\":\n        mask = MNAR_mask_logistic(har1, p_miss, p_obs).double()\n    elif mecha == \"MNAR\" and opt == \"quantile\":\n        mask = MNAR_mask_quantiles(har1, p_miss, q, 1-p_obs).double()\n    elif mecha == \"MNAR\" and opt == \"selfmasked\":\n        mask = MNAR_self_mask_logistic(har1, p_miss).double()\n    else:\n        mask = (torch.rand(har1.shape) < p_miss).double()\n     \n    \n    har1_nas = har1.clone()\n    har1_nas[mask.bool()] = np.nan\n    \n    \n    \n    \n    return {'har1_init': har1.double(), 'har1_incomp': har1_nas.double(), 'mask': mask}\n\n\n\nhar1_miss_mar = produce_NA(har1=har1, p_miss=0.20, mecha=\"MAR\", p_obs=0.5)\n \n\n\n#print(har1_mar) \nprint(har1_miss_mar['har1_init'])\nhar1_mar = har1_miss_mar['har1_incomp']\nprint(har1_mar)  \n#har1_mar = pd.DataFrame.from_dict(har1_mar)\nR_mcar = har1_miss_mar['mask']\nprint(R_mcar)\n \n\nprint(\"Percentage of generated missing values: \", (R_mcar.sum()).numpy()/np.prod(R_mcar.size())*100, \" %\")\n\nprint(har1_mar)\nprint('Shape Test:\\t{}\\n'.format(har1_mar.shape))\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\n \nx_np = har1_mar.numpy()\nx_df = pd.DataFrame(x_np)\nx_df.to_csv('10mar_test.csv')\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\n\ntdf = pd.read_csv(\"./10mar_test.csv\").iloc[:, 1:]\ntdf.head()\n\ntdf.shape\n# Get null values and dataframe information  check for any missing data in data sets\nprint('Null Values In DataFrame: {}\\n'.format(tdf.isna().sum().sum()))\ntdf.info()\ntdf=tdf.fillna(0)\ntdf.to_csv(\"data_fil100.csv\")\n\ntdf\n\n\n\ntestdata = pd.read_csv('./10mar_test.csv').iloc[:, 1:] \n \ntestdata.head(10)\n#testdata=testdata.drop(['16'],axis=1)\ntestdata\n\nprint('Null Values In DataFrame: {}\\n'.format(testdata.isna().sum().sum()))\ntestdata.isnull().sum(0)\n\ntestdata=testdata.fillna(0)\ntestdata\n\n\n\nfrom sklearn.model_selection import train_test_split\n#kfold =sklearn.model_selection.KFold(n_splits=10)\n\nimport pandas as pd\n \nfrom sklearn.metrics import log_loss\n#seed=7\n#\n\n# importing voting classifier\nfrom sklearn.ensemble import VotingClassifier\n \n    \n#Edata10Mar = supp10_data2\nEdata10Mar =testdata \nEdata10Mar\n\nX = Edata10Mar \ny= df_datab2[\"class\"] \nX\nfeature_cols = list(X) \ny\n# split data\nX_train, X_test, Y_train, Y_test = train_test_split(X, y)\n\n\n\n\n\nestimators = []\nestimators.append(('c1',clf_dtcb21))\nestimators.append(('c2',clf_dtcb22))\nestimators.append(('c3',clf_dtcb23))\nestimators.append(('c4',clf_dtcb24))\nestimators.append(('c5',clf_dtcb25))\n\n\n\n'''voting = VotingClassifier(estimators=estimators,voting='hard')\nvot_hard=voting.fit(X_train, Y_train)\n\ny_pred=vot_hard.predict(X_test)\nprint(y_pred)\nscore=accuracy_score(Y_test,y_pred)\n#print(\"hard voting score %d\" % score)\nprint(score)'''\n\n\n\n# Voting Classifier with soft voting\nvot_soft = VotingClassifier(estimators = estimators, voting ='soft')\nvot_soft.fit(X_train, Y_train)\ny_pred = vot_soft.predict(X_test)\nprint(y_pred)\n# using accuracy_score\nsscore = accuracy_score(Y_test, y_pred)\n#print(\"Soft Voting Score % d\" % score)\nprint(sscore)\n\nfrom sklearn.metrics import classification_report\n\n#print(classification_report(Y_test, y_pred))\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(Y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T06:01:13.775118Z","iopub.execute_input":"2024-01-09T06:01:13.7757Z","iopub.status.idle":"2024-01-09T06:01:13.945867Z","shell.execute_reply.started":"2024-01-09T06:01:13.77566Z","shell.execute_reply":"2024-01-09T06:01:13.943962Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport missingno as msno\n\ndf_datab2=pd.read_csv(\"/kaggle/input/b2-dataset/B2-newmodifieddataset.csv\").iloc[:, 1:]\n\ndf_datab2=df_datab2.fillna(0)\n\ndf_datab2.info()\n\n\nhar = df_datab2\n#har1=har.iloc[:,:-1]\nhar1=har\n#msno.heatmap(har1)\n#msno.dendrogram(har1)\nhar1=har1.values\n#print('Shape Test:\\t{}\\n'.format(har1.shape))\nhar1\n\n\nhar1=har1.astype(float)\nhar1\n\n\ndef produce_NA(har1, p_miss, mecha=\"MAR\", opt=\"logistic\", p_obs=None, q=None):\n    to_torch = torch.is_tensor(har1) ## output a pytorch tensor, or a numpy array\n    if not to_torch:\n        har1 = har1.astype(np.float32)\n        har1 = torch.from_numpy(har1)\n    \n    if mecha == \"MAR\" and opt == \"logistic\":\n        mask = MAR_mask(har1, p_miss, p_obs).double()\n        print(\"mask values\")\n        print(mask)\n    elif mecha == \"MNAR\" and opt == \"logistic\":\n        mask = MNAR_mask_logistic(har1, p_miss, p_obs).double()\n    elif mecha == \"MNAR\" and opt == \"quantile\":\n        mask = MNAR_mask_quantiles(har1, p_miss, q, 1-p_obs).double()\n    elif mecha == \"MNAR\" and opt == \"selfmasked\":\n        mask = MNAR_self_mask_logistic(har1, p_miss).double()\n    else:\n        mask = (torch.rand(har1.shape) < p_miss).double()\n     \n    \n    har1_nas = har1.clone()\n    har1_nas[mask.bool()] = np.nan\n    \n    \n    \n    \n    return {'har1_init': har1.double(), 'har1_incomp': har1_nas.double(), 'mask': mask}\n\n\n\nhar1_miss_mar = produce_NA(har1=har1, p_miss=0.30, mecha=\"MAR\", p_obs=0.5)\n \n\n\n#print(har1_mar) \nprint(har1_miss_mar['har1_init'])\nhar1_mar = har1_miss_mar['har1_incomp']\nprint(har1_mar)  \n#har1_mar = pd.DataFrame.from_dict(har1_mar)\nR_mcar = har1_miss_mar['mask']\nprint(R_mcar)\n \n\nprint(\"Percentage of generated missing values: \", (R_mcar.sum()).numpy()/np.prod(R_mcar.size())*100, \" %\")\n\nprint(har1_mar)\nprint('Shape Test:\\t{}\\n'.format(har1_mar.shape))\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\n \nx_np = har1_mar.numpy()\nx_df = pd.DataFrame(x_np)\nx_df.to_csv('10mar_test.csv')\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\n\ntdf = pd.read_csv(\"./10mar_test.csv\").iloc[:, 1:]\ntdf.head()\n\ntdf.shape\n# Get null values and dataframe information  check for any missing data in data sets\nprint('Null Values In DataFrame: {}\\n'.format(tdf.isna().sum().sum()))\ntdf.info()\ntdf=tdf.fillna(0)\ntdf.to_csv(\"data_fil100.csv\")\n\ntdf\n\n\n\ntestdata = pd.read_csv('./10mar_test.csv').iloc[:, 1:] \n \ntestdata.head(10)\n#testdata=testdata.drop(['16'],axis=1)\ntestdata\n\nprint('Null Values In DataFrame: {}\\n'.format(testdata.isna().sum().sum()))\ntestdata.isnull().sum(0)\n\ntestdata=testdata.fillna(0)\ntestdata\n\n\n\nfrom sklearn.model_selection import train_test_split\n#kfold =sklearn.model_selection.KFold(n_splits=10)\n\nimport pandas as pd\n \nfrom sklearn.metrics import log_loss\n#seed=7\n#\n\n# importing voting classifier\nfrom sklearn.ensemble import VotingClassifier\n \n    \n#Edata10Mar = supp10_data2\nEdata10Mar =testdata \nEdata10Mar\n\nX = Edata10Mar \ny= df_datab2[\"class\"] \nX\nfeature_cols = list(X) \ny\n# split data\nX_train, X_test, Y_train, Y_test = train_test_split(X, y)\n\n\n\n\n\nestimators = []\nestimators.append(('c1',clf_dtcb21))\nestimators.append(('c2',clf_dtcb22))\nestimators.append(('c3',clf_dtcb23))\nestimators.append(('c4',clf_dtcb24))\nestimators.append(('c5',clf_dtcb25))\n\n\n\n'''voting = VotingClassifier(estimators=estimators,voting='hard')\nvot_hard=voting.fit(X_train, Y_train)\n\ny_pred=vot_hard.predict(X_test)\nprint(y_pred)\nscore=accuracy_score(Y_test,y_pred)\n#print(\"hard voting score %d\" % score)\nprint(score)'''\n\n\n\n# Voting Classifier with soft voting\nvot_soft = VotingClassifier(estimators = estimators, voting ='soft')\nvot_soft.fit(X_train, Y_train)\ny_pred = vot_soft.predict(X_test)\nprint(y_pred)\n# using accuracy_score\nsscore = accuracy_score(Y_test, y_pred)\n#print(\"Soft Voting Score % d\" % score)\nprint(sscore)\n\nfrom sklearn.metrics import classification_report\n\n#print(classification_report(Y_test, y_pred))\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(Y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T06:02:01.73459Z","iopub.execute_input":"2024-01-09T06:02:01.735082Z","iopub.status.idle":"2024-01-09T06:02:01.891561Z","shell.execute_reply.started":"2024-01-09T06:02:01.735049Z","shell.execute_reply":"2024-01-09T06:02:01.890346Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport missingno as msno\n\ndf_datab2=pd.read_csv(\"/kaggle/input/b2-dataset/B2-newmodifieddataset.csv\").iloc[:, 1:]\n\ndf_datab2=df_datab2.fillna(0)\n\ndf_datab2.info()\n\n\nhar = df_datab2\n#har1=har.iloc[:,:-1]\nhar1=har\n#msno.heatmap(har1)\n#msno.dendrogram(har1)\nhar1=har1.values\n#print('Shape Test:\\t{}\\n'.format(har1.shape))\nhar1\n\n\nhar1=har1.astype(float)\nhar1\n\n\ndef produce_NA(har1, p_miss, mecha=\"MAR\", opt=\"logistic\", p_obs=None, q=None):\n    to_torch = torch.is_tensor(har1) ## output a pytorch tensor, or a numpy array\n    if not to_torch:\n        har1 = har1.astype(np.float32)\n        har1 = torch.from_numpy(har1)\n    \n    if mecha == \"MAR\" and opt == \"logistic\":\n        mask = MAR_mask(har1, p_miss, p_obs).double()\n        print(\"mask values\")\n        print(mask)\n    elif mecha == \"MNAR\" and opt == \"logistic\":\n        mask = MNAR_mask_logistic(har1, p_miss, p_obs).double()\n    elif mecha == \"MNAR\" and opt == \"quantile\":\n        mask = MNAR_mask_quantiles(har1, p_miss, q, 1-p_obs).double()\n    elif mecha == \"MNAR\" and opt == \"selfmasked\":\n        mask = MNAR_self_mask_logistic(har1, p_miss).double()\n    else:\n        mask = (torch.rand(har1.shape) < p_miss).double()\n     \n    \n    har1_nas = har1.clone()\n    har1_nas[mask.bool()] = np.nan\n    \n    \n    \n    \n    return {'har1_init': har1.double(), 'har1_incomp': har1_nas.double(), 'mask': mask}\n\n\n\nhar1_miss_mar = produce_NA(har1=har1, p_miss=0.40, mecha=\"MAR\", p_obs=0.5)\n \n\n\n#print(har1_mar) \nprint(har1_miss_mar['har1_init'])\nhar1_mar = har1_miss_mar['har1_incomp']\nprint(har1_mar)  \n#har1_mar = pd.DataFrame.from_dict(har1_mar)\nR_mcar = har1_miss_mar['mask']\nprint(R_mcar)\n \n\nprint(\"Percentage of generated missing values: \", (R_mcar.sum()).numpy()/np.prod(R_mcar.size())*100, \" %\")\n\nprint(har1_mar)\nprint('Shape Test:\\t{}\\n'.format(har1_mar.shape))\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\n \nx_np = har1_mar.numpy()\nx_df = pd.DataFrame(x_np)\nx_df.to_csv('10mar_test.csv')\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\n\ntdf = pd.read_csv(\"./10mar_test.csv\").iloc[:, 1:]\ntdf.head()\n\ntdf.shape\n# Get null values and dataframe information  check for any missing data in data sets\nprint('Null Values In DataFrame: {}\\n'.format(tdf.isna().sum().sum()))\ntdf.info()\ntdf=tdf.fillna(0)\ntdf.to_csv(\"data_fil100.csv\")\n\ntdf\n\n\n\ntestdata = pd.read_csv('./10mar_test.csv').iloc[:, 1:] \n \ntestdata.head(10)\n#testdata=testdata.drop(['16'],axis=1)\ntestdata\n\nprint('Null Values In DataFrame: {}\\n'.format(testdata.isna().sum().sum()))\ntestdata.isnull().sum(0)\n\ntestdata=testdata.fillna(0)\ntestdata\n\n\n\nfrom sklearn.model_selection import train_test_split\n#kfold =sklearn.model_selection.KFold(n_splits=10)\n\nimport pandas as pd\n \nfrom sklearn.metrics import log_loss\n#seed=7\n#\n\n# importing voting classifier\nfrom sklearn.ensemble import VotingClassifier\n \n    \n#Edata10Mar = supp10_data2\nEdata10Mar =testdata \nEdata10Mar\n\nX = Edata10Mar \ny= df_datab2[\"class\"] \nX\nfeature_cols = list(X) \ny\n# split data\nX_train, X_test, Y_train, Y_test = train_test_split(X, y)\n\n\n\n\n\nestimators = []\nestimators.append(('c1',clf_dtcb21))\nestimators.append(('c2',clf_dtcb22))\nestimators.append(('c3',clf_dtcb23))\nestimators.append(('c4',clf_dtcb24))\nestimators.append(('c5',clf_dtcb25))\n\n\n\n'''voting = VotingClassifier(estimators=estimators,voting='hard')\nvot_hard=voting.fit(X_train, Y_train)\n\ny_pred=vot_hard.predict(X_test)\nprint(y_pred)\nscore=accuracy_score(Y_test,y_pred)\n#print(\"hard voting score %d\" % score)\nprint(score)'''\n\n\n\n# Voting Classifier with soft voting\nvot_soft = VotingClassifier(estimators = estimators, voting ='soft')\nvot_soft.fit(X_train, Y_train)\ny_pred = vot_soft.predict(X_test)\nprint(y_pred)\n# using accuracy_score\nsscore = accuracy_score(Y_test, y_pred)\n#print(\"Soft Voting Score % d\" % score)\nprint(sscore)\n\nfrom sklearn.metrics import classification_report\n\n#print(classification_report(Y_test, y_pred))\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(Y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T06:02:16.476685Z","iopub.execute_input":"2024-01-09T06:02:16.477182Z","iopub.status.idle":"2024-01-09T06:02:16.631424Z","shell.execute_reply.started":"2024-01-09T06:02:16.477146Z","shell.execute_reply":"2024-01-09T06:02:16.630249Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport missingno as msno\n\ndf_datab2=pd.read_csv(\"/kaggle/input/b2-dataset/B2-newmodifieddataset.csv\").iloc[:, 1:]\n\ndf_datab2=df_datab2.fillna(0)\n\ndf_datab2.info()\n\n\nhar = df_datab2\n#har1=har.iloc[:,:-1]\nhar1=har\n#msno.heatmap(har1)\n#msno.dendrogram(har1)\nhar1=har1.values\n#print('Shape Test:\\t{}\\n'.format(har1.shape))\nhar1\n\n\nhar1=har1.astype(float)\nhar1\n\n\ndef produce_NA(har1, p_miss, mecha=\"MAR\", opt=\"logistic\", p_obs=None, q=None):\n    to_torch = torch.is_tensor(har1) ## output a pytorch tensor, or a numpy array\n    if not to_torch:\n        har1 = har1.astype(np.float32)\n        har1 = torch.from_numpy(har1)\n    \n    if mecha == \"MAR\" and opt == \"logistic\":\n        mask = MAR_mask(har1, p_miss, p_obs).double()\n        print(\"mask values\")\n        print(mask)\n    elif mecha == \"MNAR\" and opt == \"logistic\":\n        mask = MNAR_mask_logistic(har1, p_miss, p_obs).double()\n    elif mecha == \"MNAR\" and opt == \"quantile\":\n        mask = MNAR_mask_quantiles(har1, p_miss, q, 1-p_obs).double()\n    elif mecha == \"MNAR\" and opt == \"selfmasked\":\n        mask = MNAR_self_mask_logistic(har1, p_miss).double()\n    else:\n        mask = (torch.rand(har1.shape) < p_miss).double()\n     \n    \n    har1_nas = har1.clone()\n    har1_nas[mask.bool()] = np.nan\n    \n    \n    \n    \n    return {'har1_init': har1.double(), 'har1_incomp': har1_nas.double(), 'mask': mask}\n\n\n\nhar1_miss_mar = produce_NA(har1=har1, p_miss=0.50, mecha=\"MAR\", p_obs=0.5)\n \n\n\n#print(har1_mar) \nprint(har1_miss_mar['har1_init'])\nhar1_mar = har1_miss_mar['har1_incomp']\nprint(har1_mar)  \n#har1_mar = pd.DataFrame.from_dict(har1_mar)\nR_mcar = har1_miss_mar['mask']\nprint(R_mcar)\n \n\nprint(\"Percentage of generated missing values: \", (R_mcar.sum()).numpy()/np.prod(R_mcar.size())*100, \" %\")\n\nprint(har1_mar)\nprint('Shape Test:\\t{}\\n'.format(har1_mar.shape))\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\n \nx_np = har1_mar.numpy()\nx_df = pd.DataFrame(x_np)\nx_df.to_csv('10mar_test.csv')\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\n\ntdf = pd.read_csv(\"./10mar_test.csv\").iloc[:, 1:]\ntdf.head()\n\ntdf.shape\n# Get null values and dataframe information  check for any missing data in data sets\nprint('Null Values In DataFrame: {}\\n'.format(tdf.isna().sum().sum()))\ntdf.info()\ntdf=tdf.fillna(0)\ntdf.to_csv(\"data_fil100.csv\")\n\ntdf\n\n\n\ntestdata = pd.read_csv('./10mar_test.csv').iloc[:, 1:] \n \ntestdata.head(10)\n#testdata=testdata.drop(['16'],axis=1)\ntestdata\n\nprint('Null Values In DataFrame: {}\\n'.format(testdata.isna().sum().sum()))\ntestdata.isnull().sum(0)\n\ntestdata=testdata.fillna(0)\ntestdata\n\n\n\nfrom sklearn.model_selection import train_test_split\n#kfold =sklearn.model_selection.KFold(n_splits=10)\n\nimport pandas as pd\n \nfrom sklearn.metrics import log_loss\n#seed=7\n#\n\n# importing voting classifier\nfrom sklearn.ensemble import VotingClassifier\n \n    \n#Edata10Mar = supp10_data2\nEdata10Mar =testdata \nEdata10Mar\n\nX = Edata10Mar \ny= df_datab2[\"class\"] \nX\nfeature_cols = list(X) \ny\n# split data\nX_train, X_test, Y_train, Y_test = train_test_split(X, y)\n\n\n\n\n\nestimators = []\nestimators.append(('c1',clf_dtcb21))\nestimators.append(('c2',clf_dtcb22))\nestimators.append(('c3',clf_dtcb23))\nestimators.append(('c4',clf_dtcb24))\nestimators.append(('c5',clf_dtcb25))\n\n\n\n'''voting = VotingClassifier(estimators=estimators,voting='hard')\nvot_hard=voting.fit(X_train, Y_train)\n\ny_pred=vot_hard.predict(X_test)\nprint(y_pred)\nscore=accuracy_score(Y_test,y_pred)\n#print(\"hard voting score %d\" % score)\nprint(score)'''\n\n\n\n# Voting Classifier with soft voting\nvot_soft = VotingClassifier(estimators = estimators, voting ='soft')\nvot_soft.fit(X_train, Y_train)\ny_pred = vot_soft.predict(X_test)\nprint(y_pred)\n# using accuracy_score\nsscore = accuracy_score(Y_test, y_pred)\n#print(\"Soft Voting Score % d\" % score)\nprint(sscore)\n\nfrom sklearn.metrics import classification_report\n\n#print(classification_report(Y_test, y_pred))\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(Y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T06:02:31.938676Z","iopub.execute_input":"2024-01-09T06:02:31.93928Z","iopub.status.idle":"2024-01-09T06:02:32.095295Z","shell.execute_reply.started":"2024-01-09T06:02:31.939242Z","shell.execute_reply":"2024-01-09T06:02:32.094065Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Now Lets combine both the balanced datasets EC's and make EC of EC and test with different percentages of missing values \n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport missingno as msno\n\ndf_datab2=pd.read_csv(\"/kaggle/input/actual-after-preprocesing-datast/actual-newmodifieddataset.csv\").iloc[:, 1:]\n\ndf_datab2=df_datab2.fillna(0)\n\ndf_datab2.info()\n\n\nhar = df_datab2\n#har1=har.iloc[:,:-1]\nhar1=har\n#msno.heatmap(har1)\n#msno.dendrogram(har1)\nhar1=har1.values\n#print('Shape Test:\\t{}\\n'.format(har1.shape))\nhar1\n\n\nhar1=har1.astype(float)\nhar1\n\n\ndef produce_NA(har1, p_miss, mecha=\"MAR\", opt=\"logistic\", p_obs=None, q=None):\n    to_torch = torch.is_tensor(har1) ## output a pytorch tensor, or a numpy array\n    if not to_torch:\n        har1 = har1.astype(np.float32)\n        har1 = torch.from_numpy(har1)\n    \n    if mecha == \"MAR\" and opt == \"logistic\":\n        mask = MAR_mask(har1, p_miss, p_obs).double()\n        print(\"mask values\")\n        print(mask)\n    elif mecha == \"MNAR\" and opt == \"logistic\":\n        mask = MNAR_mask_logistic(har1, p_miss, p_obs).double()\n    elif mecha == \"MNAR\" and opt == \"quantile\":\n        mask = MNAR_mask_quantiles(har1, p_miss, q, 1-p_obs).double()\n    elif mecha == \"MNAR\" and opt == \"selfmasked\":\n        mask = MNAR_self_mask_logistic(har1, p_miss).double()\n    else:\n        mask = (torch.rand(har1.shape) < p_miss).double()\n     \n    \n    har1_nas = har1.clone()\n    har1_nas[mask.bool()] = np.nan\n    \n    \n    \n    \n    return {'har1_init': har1.double(), 'har1_incomp': har1_nas.double(), 'mask': mask}\n\n\n\nhar1_miss_mar = produce_NA(har1=har1, p_miss=0.40, mecha=\"MAR\", p_obs=0.5)\n \n\n\n#print(har1_mar) \nprint(har1_miss_mar['har1_init'])\nhar1_mar = har1_miss_mar['har1_incomp']\nprint(har1_mar)  \n#har1_mar = pd.DataFrame.from_dict(har1_mar)\nR_mcar = har1_miss_mar['mask']\nprint(R_mcar)\n \n\nprint(\"Percentage of generated missing values: \", (R_mcar.sum()).numpy()/np.prod(R_mcar.size())*100, \" %\")\n\nprint(har1_mar)\nprint('Shape Test:\\t{}\\n'.format(har1_mar.shape))\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\n \nx_np = har1_mar.numpy()\nx_df = pd.DataFrame(x_np)\nx_df.to_csv('10mar_test.csv')\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\n\ntdf = pd.read_csv(\"./10mar_test.csv\").iloc[:, 1:]\ntdf.head()\n\ntdf.shape\n# Get null values and dataframe information  check for any missing data in data sets\nprint('Null Values In DataFrame: {}\\n'.format(tdf.isna().sum().sum()))\ntdf.info()\ntdf=tdf.fillna(0)\ntdf.to_csv(\"data_fil100.csv\")\n\ntdf\n\n\n\ntestdata = pd.read_csv('./10mar_test.csv').iloc[:, 1:] \n \ntestdata.head(10)\n#testdata=testdata.drop(['16'],axis=1)\ntestdata\n\nprint('Null Values In DataFrame: {}\\n'.format(testdata.isna().sum().sum()))\ntestdata.isnull().sum(0)\n\ntestdata=testdata.fillna(0)\ntestdata\n\n\n\nfrom sklearn.model_selection import train_test_split\n#kfold =sklearn.model_selection.KFold(n_splits=10)\n\nimport pandas as pd\n \nfrom sklearn.metrics import log_loss\n#seed=7\n#\n\n# importing voting classifier\nfrom sklearn.ensemble import VotingClassifier\n \n    \n#Edata10Mar = supp10_data2\nEdata10Mar =testdata \nEdata10Mar\n\nX = Edata10Mar \ny= df_datab2[\"class\"] \nX\nfeature_cols = list(X) \ny\n# split data\nX_train, X_test, Y_train, Y_test = train_test_split(X, y)\n\n\n\n\n\nestimators = []\nestimators.append(('c1',clf_dtc2))\nestimators.append(('c2',clf_dtc3))\nestimators.append(('c3',clf_dtc4))\nestimators.append(('c4',clf_dtc5))\nestimators.append(('c5',clf_dtcb21))\nestimators.append(('c6',clf_dtcb22))\nestimators.append(('c7',clf_dtcb23))\nestimators.append(('c8',clf_dtcb24))\nestimators.append(('c9',clf_dtcb25))\n\n\n\n'''voting = VotingClassifier(estimators=estimators,voting='hard')\nvot_hard=voting.fit(X_train, Y_train)\n\ny_pred=vot_hard.predict(X_test)\nprint(y_pred)\nscore=accuracy_score(Y_test,y_pred)\n#print(\"hard voting score %d\" % score)\nprint(score)'''\n\n\n\n# Voting Classifier with soft voting\nvot_soft = VotingClassifier(estimators = estimators, voting ='soft')\nvot_soft.fit(X_train, Y_train)\ny_pred = vot_soft.predict(X_test)\nprint(y_pred)\n# using accuracy_score\nsscore = accuracy_score(Y_test, y_pred)\n#print(\"Soft Voting Score % d\" % score)\nprint(sscore)\n\nfrom sklearn.metrics import classification_report\n\n#print(classification_report(Y_test, y_pred))\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(Y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T06:12:49.003695Z","iopub.execute_input":"2024-01-09T06:12:49.004202Z","iopub.status.idle":"2024-01-09T06:12:49.22421Z","shell.execute_reply.started":"2024-01-09T06:12:49.00417Z","shell.execute_reply":"2024-01-09T06:12:49.222833Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Now apply other classifiers on the actual data ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport missingno as msno\n\ndf_datab2=pd.read_csv(\"/kaggle/input/actual-after-preprocesing-datast/actual-newmodifieddataset.csv\").iloc[:, 1:]\n\ndf_datab2=df_datab2.fillna(0)\n\ndf_datab2.info()\n\norig_dataset=df_datab2.drop('class',axis=1)\norig_dataset\nimport pandas as pd\nimport missingno as msno\nhar = orig_dataset\n#har1=har.iloc[:,:-1]\nhar1=har\n#msno.heatmap(har1)\n#msno.dendrogram(har1)\nhar1=har1.values\n#print('Shape Test:\\t{}\\n'.format(har1.shape))\nhar1\n\n\nhar1=har1.astype(float)\nhar1\n\nhar1_miss_mar = produce_NA(har1=har1, p_miss=0.10, mecha=\"MAR\", p_obs=0.5)\n \n\n\n#print(har1_mar) \nprint(har1_miss_mar['har1_init'])\nhar1_mar = har1_miss_mar['har1_incomp']\nprint(har1_mar)  \n#har1_mar = pd.DataFrame.from_dict(har1_mar)\nR_mcar = har1_miss_mar['mask']\nprint(R_mcar)\n \n\nprint(\"Percentage of generated missing values: \", (R_mcar.sum()).numpy()/np.prod(R_mcar.size())*100, \" %\")\n\nprint(har1_mar)\nprint('Shape Test:\\t{}\\n'.format(har1_mar.shape))\n\nimport torch\nimport pandas as pd\nimport numpy as np\n\n \nx_np = har1_mar.numpy()\nx_df = pd.DataFrame(x_np)\nx_df.to_csv('10mar_test.csv')\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\n\ntdf = pd.read_csv(\"./10mar_test.csv\").iloc[:, 1:]\ntdf.head()\n\ntdf.shape\n# Get null values and dataframe information  check for any missing data in data sets\nprint('Null Values In DataFrame: {}\\n'.format(tdf.isna().sum().sum()))\ntdf.info()\ntdf=tdf.fillna(0)\ntdf.to_csv(\"data_fil10.csv\")\n\ntdf\n\n\n\ntestdata = pd.read_csv('./10mar_test.csv').iloc[:, 1:] \n \ntestdata.head(10)\n#testdata=testdata.drop(['16'],axis=1)\ntestdata\n\nprint('Null Values In DataFrame: {}\\n'.format(testdata.isna().sum().sum()))\ntestdata.isnull().sum(0)\n\ntestdata=testdata.fillna(0)\ntestdata\nfrom sklearn.model_selection import train_test_split\n#kfold =sklearn.model_selection.KFold(n_splits=10)\n\nimport pandas as pd\n \nfrom sklearn.metrics import log_loss\n#seed=7\n#\n\n# importing voting classifier\nfrom sklearn.ensemble import VotingClassifier\n \n    \n#Edata10Mar = supp10_data2\nEdata10Mar =testdata \nEdata10Mar\nX = Edata10Mar  \ny= df_datab2[\"class\"] \nX\nfeature_cols = list(X) \ny\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = None)\n\nclf_dtc5 = DecisionTreeClassifier()\nclf_dtc5.fit(X_train, y_train)\n\n#clf_dtc5 = DecisionTreeClassifier()\n#clf_dtc5.fit(X_train, y_train)\n\nfeature_cols = list(X) \n\n\n\n\ny_pred = clf_dtc5.predict(X_test)\nprint(y_pred)\ndef Model_Performance(test,pred):\n    #precision = precision_score(test,pred)\n    precision = precision_score(test,pred,average='macro')\n    recall = recall_score(test,pred,average='macro')\n    f1 = f1_score(test,pred,average='macro')\n    #print('1. Confusion Matrix:\\n',confusion_matrix(test, pred))\n    print(\"\\n2. Accuracy Score:\", round(accuracy_score(test, pred)*100,2),\"%\")\n    print(\"3. Precision:\", round(precision*100,2),\"%\")\n    print(\"4. Recall:\",round(recall*100,2),\"%\" )\n    print(\"5. F1 Score:\",round(f1*100,2),\"%\" )\n    print(\"6. clasification report:\\n\",classification_report(test, pred))\nModel_Performance(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T06:58:21.867055Z","iopub.execute_input":"2024-01-09T06:58:21.867552Z","iopub.status.idle":"2024-01-09T06:58:22.021522Z","shell.execute_reply.started":"2024-01-09T06:58:21.867515Z","shell.execute_reply":"2024-01-09T06:58:22.019725Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}